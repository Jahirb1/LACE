{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArQX1NBBe01B"
   },
   "source": [
    "# LACE - Logic-Aware Consistency Evaluation\n",
    "\n",
    "A comprehensive evaluation framework for measuring LLM logical consistency across semantic, formal, and knowledge-grounding dimensions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the LACE (Logic-Aware Consistency Evaluation) framework, which evaluates language models on six complementary metrics:\n",
    "\n",
    "| Metric | Description | Dataset |\n",
    "|--------|-------------|---------|\n",
    "| EC | Entailment Classification | EntailmentBank |\n",
    "| CR | Contradiction Rate | EntailmentBank |\n",
    "| RS_F | Relational Structure (Formal) | RuleTaker |\n",
    "| AC | Answer Consistency | RuleTaker |\n",
    "| KAS | Knowledge Attribute Scoring | RuleTaker |\n",
    "| RS_KG | Relational Structure (Knowledge Graph) | WebNLG |\n",
    "\n",
    "The final **LCS (Logic Consistency Score)** is a weighted combination of these metrics.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Increased sample sizes for better statistical significance\n",
    "- Harder RuleTaker examples filtered by rule count\n",
    "- Weighted LCS formula emphasizing discriminative metrics\n",
    "- 4-bit quantization support for larger models\n",
    "- CUDA-safe greedy decoding to avoid synchronization errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qud04GNce01D"
   },
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "Run the installation cell below, then restart the runtime before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82S-EWqBe01E"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LACE - Dependency Installation\n",
    "# =============================================================================\n",
    "# Install required packages for the evaluation framework.\n",
    "# Run this cell once, then restart the runtime before proceeding.\n",
    "#\n",
    "# Package descriptions:\n",
    "#   - datasets: HuggingFace datasets library for loading benchmark data\n",
    "#   - transformers: Model loading and tokenization\n",
    "#   - accelerate: Efficient model distribution across devices\n",
    "#   - sentencepiece: Tokenizer backend for certain models\n",
    "#   - bitsandbytes: 4-bit quantization support for large models\n",
    "#   - tqdm: Progress bar visualization\n",
    "#   - matplotlib/pandas: Results visualization and data handling\n",
    "#   - numpy: Numerical computations\n",
    "# =============================================================================\n",
    "\n",
    "!pip install -q \"datasets==2.19.1\" \"transformers>=4.40\" accelerate sentencepiece bitsandbytes tqdm matplotlib pandas \"numpy<2.0\"\n",
    "\n",
    "# Note: bitsandbytes enables 4-bit quantization for Phi-3-mini and Mistral-7B,\n",
    "# allowing these larger models to run on consumer GPUs with limited VRAM.\n",
    "# After installation completes, restart the runtime before running subsequent cells.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5GJSH-Fe01E",
    "outputId": "0ab09279-4884-478e-c24c-b64352773dc4"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Import Libraries and Configure Device\n",
    "# =============================================================================\n",
    "# This cell imports all required libraries and sets up the compute device.\n",
    "# CUDA GPU is preferred for faster inference; CPU is used as fallback.\n",
    "# =============================================================================\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clear any existing CUDA state to ensure clean memory allocation\n",
    "# -----------------------------------------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Device Configuration\n",
    "# Select GPU if available, otherwise fall back to CPU\n",
    "# -----------------------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device Configuration\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  Memory: {mem_gb:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6CikwAte01F"
   },
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define evaluation parameters including models, sample sizes, and metric weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN05fF7Se01G",
    "outputId": "df0cb8ef-906e-42f9-d1d9-a7365c74aced"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Evaluation Configuration\n",
    "# =============================================================================\n",
    "# This dataclass contains all configurable parameters for the LACE evaluation.\n",
    "# Modify these values to customize the evaluation behavior.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LACEConfig:\n",
    "    \"\"\"\n",
    "    Configuration settings for LACE (Logic-Aware Consistency Evaluation).\n",
    "\n",
    "    This dataclass encapsulates all parameters needed to run the evaluation,\n",
    "    including model selection, dataset sizes, generation parameters, and\n",
    "    scoring weights.\n",
    "\n",
    "    Attributes:\n",
    "        models_to_test: List of HuggingFace model identifiers to evaluate.\n",
    "        quantized_models: Subset of models requiring 4-bit quantization.\n",
    "        nli_model: Model identifier for the NLI judge.\n",
    "        num_samples: Number of samples for EntailmentBank and WebNLG.\n",
    "        num_ruletaker_samples: Number of samples for RuleTaker dataset.\n",
    "        min_ruletaker_rules: Minimum rules required for RuleTaker filtering.\n",
    "        lcs_weights: Metric weights for computing the final LCS score.\n",
    "        num_generations: Number of generations per prompt (for greedy).\n",
    "        num_consistency_generations: Number of prompt variations for AC.\n",
    "        max_new_tokens: Maximum tokens to generate per response.\n",
    "        temperature: Sampling temperature (1.0 = deterministic).\n",
    "        top_p: Nucleus sampling parameter.\n",
    "        repetition_penalty: Penalty for repeated tokens.\n",
    "        seed: Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Model Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    # List of models to evaluate, ordered by parameter count (ascending)\n",
    "    models_to_test: List[str] = field(default_factory=lambda: [\n",
    "        \"Qwen/Qwen2.5-0.5B-Instruct\",          # 0.5 billion parameters\n",
    "        \"Qwen/Qwen2.5-1.5B-Instruct\",          # 1.5 billion parameters\n",
    "        \"microsoft/Phi-3-mini-4k-instruct\",    # 3.8 billion parameters\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\",  # 7 billion parameters\n",
    "    ])\n",
    "\n",
    "    # Models that require 4-bit quantization to fit in GPU memory\n",
    "    quantized_models: List[str] = field(default_factory=lambda: [\n",
    "        \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    ])\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NLI Judge Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    # RoBERTa-large fine-tuned on MNLI for entailment/contradiction detection\n",
    "    nli_model: str = \"roberta-large-mnli\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Dataset Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sample sizes chosen to balance statistical significance with runtime\n",
    "    num_samples: int = 50                # EntailmentBank and WebNLG samples\n",
    "    num_ruletaker_samples: int = 100     # RuleTaker samples (logic-focused)\n",
    "    min_ruletaker_rules: int = 5         # Minimum rules for harder examples\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # LCS Weighting Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Weights emphasize discriminative metrics (EC, RS_F, KAS) over\n",
    "    # near-ceiling (AC) or near-floor (CR) metrics\n",
    "    lcs_weights: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"ec\": 2.0,      # Entailment Classification: highly discriminative\n",
    "        \"cr\": 0.5,      # Contradiction Rate: near-floor, less informative\n",
    "        \"ac\": 0.5,      # Answer Consistency: near-ceiling, less informative\n",
    "        \"rs_f\": 2.0,    # Relational Structure (Formal): highly discriminative\n",
    "        \"rs_kg\": 1.0,   # Knowledge Graph Relation: moderately discriminative\n",
    "        \"kas\": 2.0,     # Counterfactual Sensitivity: highly discriminative\n",
    "    })\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generation Parameters\n",
    "    # -------------------------------------------------------------------------\n",
    "    num_generations: int = 1             # Generations per prompt (greedy mode)\n",
    "    num_consistency_generations: int = 4 # Prompt variations for AC metric\n",
    "    max_new_tokens: int = 64             # Maximum response length\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sampling Parameters\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Note: Sampling is disabled (temperature=1.0) for deterministic outputs\n",
    "    # and to avoid CUDA synchronization errors with certain models\n",
    "    temperature: float = 1.0\n",
    "    top_p: float = 1.0\n",
    "    repetition_penalty: float = 1.0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Reproducibility\n",
    "    # -------------------------------------------------------------------------\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "def set_seeds(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "\n",
    "    Args:\n",
    "        seed: Integer seed value for random number generators.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Initialize Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "CONFIG = LACEConfig()\n",
    "set_seeds(CONFIG.seed)\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"Configuration Loaded\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Models: {len(CONFIG.models_to_test)} models to evaluate\")\n",
    "for model in CONFIG.models_to_test:\n",
    "    quant_tag = \" (4-bit)\" if model in CONFIG.quantized_models else \"\"\n",
    "    print(f\"    - {model}{quant_tag}\")\n",
    "print(f\"  Samples: EntailmentBank/WebNLG={CONFIG.num_samples}, \"\n",
    "      f\"RuleTaker={CONFIG.num_ruletaker_samples}\")\n",
    "print(f\"  LCS Weights: {CONFIG.lcs_weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEF65Xgye01G"
   },
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "Text processing utilities and boolean extraction from model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oz6LycERe01H",
    "outputId": "0cd64924-84b0-44c2-bf0d-fa798f67af62"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Text Processing and Boolean Extraction Utilities\n",
    "# =============================================================================\n",
    "# This module provides utility functions for:\n",
    "#   1. Converting various data types to strings\n",
    "#   2. Normalizing entity strings for comparison\n",
    "#   3. Extracting boolean answers from model outputs\n",
    "#   4. Creating counterfactual perturbations for sensitivity testing\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Text Conversion Utilities\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def to_text(value: Any) -> str:\n",
    "    \"\"\"\n",
    "    Convert various data types to a string representation.\n",
    "\n",
    "    Handles None, lists, and other types gracefully. For lists, extracts\n",
    "    the first element recursively.\n",
    "\n",
    "    Args:\n",
    "        value: Input value of any type (None, list, str, int, etc.).\n",
    "\n",
    "    Returns:\n",
    "        String representation of the input value.\n",
    "        Returns empty string for None or empty lists.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    if isinstance(value, list):\n",
    "        return to_text(value[0]) if value else \"\"\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def normalize_entity(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize entity strings for consistent matching.\n",
    "\n",
    "    Applies the following transformations:\n",
    "      1. Convert to lowercase\n",
    "      2. Replace underscores with spaces\n",
    "      3. Remove non-alphanumeric characters (except spaces)\n",
    "      4. Collapse multiple spaces to single space\n",
    "\n",
    "    Args:\n",
    "        s: Input entity string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        Normalized string suitable for comparison.\n",
    "    \"\"\"\n",
    "    s = (s or \"\").lower().replace(\"_\", \" \")\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Boolean Extraction from Model Outputs\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Pre-compiled regex patterns for efficient repeated matching\n",
    "ANSWER_LINE_RE = re.compile(\n",
    "    r\"\\b(?:final\\s+answer|final|answer)\\s*[:\\-]\\s*(true|false|yes|no)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "LAST_LINE_BOOL_RE = re.compile(\n",
    "    r\"^\\s*(true|false|yes|no)\\s*[\\.!]*\\s*$\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "BOOL_TRUE_RE = re.compile(r\"\\b(true|yes)\\b\", flags=re.IGNORECASE)\n",
    "BOOL_FALSE_RE = re.compile(r\"\\b(false|no)\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_final_boolean(text: str) -> Optional[bool]:\n",
    "    \"\"\"\n",
    "    Extract the final boolean decision from model output text.\n",
    "\n",
    "    Uses a priority-based extraction strategy:\n",
    "      1. Look for explicit markers like \"Answer: True\" or \"Final Answer: False\"\n",
    "      2. Check if the last line is a standalone boolean (True/False/Yes/No)\n",
    "      3. Find the last occurrence of any boolean token in the text\n",
    "\n",
    "    Args:\n",
    "        text: Raw model output text to parse.\n",
    "\n",
    "    Returns:\n",
    "        Boolean value (True/False) if found, None if no boolean detected.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Priority 1: Look for explicit answer markers (e.g., \"Answer: True\")\n",
    "    matches = list(ANSWER_LINE_RE.finditer(text))\n",
    "    if matches:\n",
    "        last_match = matches[-1].group(1).lower()\n",
    "        return last_match in (\"true\", \"yes\")\n",
    "\n",
    "    # Priority 2: Check if last non-empty line is a pure boolean value\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    if lines:\n",
    "        match = LAST_LINE_BOOL_RE.match(lines[-1])\n",
    "        if match:\n",
    "            return match.group(1).lower() in (\"true\", \"yes\")\n",
    "\n",
    "    # Priority 3: Find last occurrence of any boolean token\n",
    "    true_positions = [m.start() for m in BOOL_TRUE_RE.finditer(text)]\n",
    "    false_positions = [m.start() for m in BOOL_FALSE_RE.finditer(text)]\n",
    "\n",
    "    if not true_positions and not false_positions:\n",
    "        return None\n",
    "\n",
    "    last_true = true_positions[-1] if true_positions else -1\n",
    "    last_false = false_positions[-1] if false_positions else -1\n",
    "    return last_true > last_false\n",
    "\n",
    "\n",
    "def normalize_ruletaker_label(label: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Convert RuleTaker dataset labels to boolean values.\n",
    "\n",
    "    Handles various label formats used in the dataset:\n",
    "      - Boolean values (True/False)\n",
    "      - Integer values (0/1)\n",
    "      - String values (\"true\", \"false\", \"yes\", \"no\", \"entails\", etc.)\n",
    "\n",
    "    Args:\n",
    "        label: Raw label value from RuleTaker dataset.\n",
    "\n",
    "    Returns:\n",
    "        Boolean interpretation of the label. Defaults to False for\n",
    "        unrecognized formats.\n",
    "    \"\"\"\n",
    "    if isinstance(label, bool):\n",
    "        return label\n",
    "    if isinstance(label, (int, np.integer)):\n",
    "        return bool(label)\n",
    "    if isinstance(label, str):\n",
    "        s = label.strip().lower()\n",
    "        if s in {\"true\", \"yes\", \"1\", \"entails\", \"entailed\"}:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Counterfactual Perturbation for Sensitivity Testing\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def perturb_context(context: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a counterfactual version of context by negating statements.\n",
    "\n",
    "    Used for the Knowledge Attribute Scoring (KAS) metric to test whether\n",
    "    models appropriately change their answers when premises are negated.\n",
    "\n",
    "    Transformation: Prepends \"It is not the case that\" to each sentence\n",
    "    (unless already negated).\n",
    "\n",
    "    Args:\n",
    "        context: Original context string containing logical statements.\n",
    "\n",
    "    Returns:\n",
    "        Modified context with all statements negated.\n",
    "    \"\"\"\n",
    "    context = (context or \"\").strip()\n",
    "    if not context:\n",
    "        return \"\"\n",
    "\n",
    "    # Split context into individual sentences/statements\n",
    "    parts = [p.strip() for p in re.split(r\"(?<=[.?!])\\s+|\\n+\", context) if p.strip()]\n",
    "    negated = []\n",
    "\n",
    "    for part in parts:\n",
    "        # Avoid double negation\n",
    "        if part.lower().startswith(\"it is not the case that\"):\n",
    "            negated.append(part)\n",
    "        else:\n",
    "            negated.append(f\"It is not the case that {part}\")\n",
    "\n",
    "    return \" \".join(negated)\n",
    "\n",
    "\n",
    "print(\"Utility functions loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8ElOVcZe01I"
   },
   "source": [
    "## 4. NLI Judge\n",
    "\n",
    "Natural Language Inference model for entailment and contradiction detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "496a031e6fe54fbd857fef61ed649068",
      "3f6f8b5fe013454188d07740dab29fb9",
      "e0c70acb17d94844bb79730a0f48db7d",
      "2f7de7a6af414186adfb048b4cf949e2",
      "be46c2e5c4fa4ffb9bd673f53e63bb23",
      "87aa229269bd47ef925f700c8ed8cc23",
      "45c3dd93fbe644b9ada292c12e1499a4",
      "cff1d65955c84182baf5a8c22d876d26",
      "238ad3fc92454da087e9f0a81be4742d",
      "0aba07a6382a4ffaa61f7176d25ae626",
      "8f7b0f9bd41a4dd28f49a8fb2c76700d",
      "3b995aac3e8d46549fb4bf850465742a",
      "caa65dd0abd747e784585742fb26002b",
      "3573c1917f9e4f8c8f15f1d1d24d35ec",
      "d103573e4ac04e13b9b8c0a2a61ceadc",
      "ec59fa5d82b64cdea85805f121ba10ba",
      "de6fac53bc8d47af8db45769191c7736",
      "0683af91fe8f4715a1d3c62961e71191",
      "2dee248c4b4540eaacbcc0154b1394f1",
      "e163eeb410244a7bbc0746c40902821e",
      "d0659e24caf34745a223c6159ba63abe",
      "36928b0efaff48b48937e1ccc675cae4",
      "ec320f71d8f449de9a3a8a1faacebf7b",
      "9d2f34d053274c138f77cf0dc5808d44",
      "01831fc854ee433f975cfdf11131ecfd",
      "824363c0271c4853b84cee5414ad8cd7",
      "ccc95f33c8ae447981cbd43f29466c9f",
      "2e0d27ee1ef04f21a9b9d36720d8e0b6",
      "0f459a7a09df4edc9b5a37dbe5323ff6",
      "5cc528b2d30f48d1af2dcc2943642dcc",
      "32848e92e8864283913a42d16ac66e2e",
      "44e9753fd26646d18c8d6997b89d3096",
      "1d3d3b484f6045908570c7d0bbd4ec70",
      "e0e7ad3bc9124955a7cc18d51fb6a0a1",
      "7ea63e287bef47f6bd40ff0cb6b0d84f",
      "34cc35c16fd6404690f0d6cf63c73fc6",
      "460a0ca3801d4d7d8603b4c03a015270",
      "b654725f2fee48abba6d3a266bbec616",
      "e73ae3341b3543f694f6e3fedf1bbb9a",
      "bbcd0a9a130442c1afc01613f268f852",
      "357e1f6f4831425abf0b988bdc0cf75d",
      "a6803678388843a3aaac804f8ddd453a",
      "a91e63d757c949a99b85911d978a066b",
      "1776b9d2130b465a9c1520c978599bf3",
      "b9ec721dbd9a4c0989469dd7ad45d8de",
      "8f7a69d513494bb386589a7bfe98597e",
      "ab328ffbfc914fe0b407d6c1f4bc55f4",
      "56575bcddcc64e29985a8a667c80049a",
      "19854615b117419e9645cca558b3811f",
      "065808d2a8a248f0a56d38592a79c61f",
      "8b4e6067f04441899dcde32c7f1eae76",
      "666e9c50488448f2b4fd7b4a9925874b",
      "daa5463b4c78418db4c54612bb58cd98",
      "16271db983664aaa8636511416599482",
      "93a1a93914a14c7dafeed41edc5adeb6",
      "4065ab2e5c794d3090eb66f19ffd38dc",
      "fb6932fe14ea49d5aaac53f3bd36dbf8",
      "4c4a0f4a10a44f9da513df115f64bdf3",
      "e4b23cb44e674130837f0d77b8a3daae",
      "00130ee43d6f4abe9675eda8ff4ab50b",
      "0eb58404b0354858966744dff0e728c6",
      "8616830cb8b64f1f8db60e44dbf14a3a",
      "942d8b6938b7476d97f0d81aacea723d",
      "aa42e12e9b6e448bb152af49fd759f73",
      "a1b8bba44c3a4d7e81437cd8340738fc",
      "4a044d9b934b4f25bee3237eda971c53"
     ]
    },
    "id": "6iJle3fSe01J",
    "outputId": "e2011431-383b-48ff-dfc3-a40a75ff087c"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Natural Language Inference (NLI) Judge\n",
    "# =============================================================================\n",
    "# The NLI Judge uses a pre-trained RoBERTa model fine-tuned on the MultiNLI\n",
    "# dataset to classify premise-hypothesis pairs as:\n",
    "#   - \"entailment\": hypothesis follows from premise\n",
    "#   - \"neutral\": hypothesis is neither supported nor contradicted\n",
    "#   - \"contradiction\": hypothesis contradicts the premise\n",
    "#\n",
    "# This is used for:\n",
    "#   - EC (Entailment Classification): Check if model output entails gold answer\n",
    "#   - CR (Contradiction Rate): Check if output contradicts the context\n",
    "#   - RS_KG: Check if generated text entails knowledge graph triples\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class NLIJudge:\n",
    "    \"\"\"\n",
    "    Wrapper class for Natural Language Inference classification.\n",
    "\n",
    "    Encapsulates a pre-trained sequence classification model for predicting\n",
    "    textual entailment relationships between premise-hypothesis pairs.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer: HuggingFace tokenizer for the NLI model.\n",
    "        model: Pre-trained sequence classification model.\n",
    "        id2label: Mapping from prediction indices to label strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"roberta-large-mnli\"):\n",
    "        \"\"\"\n",
    "        Initialize the NLI Judge with a pre-trained model.\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier for the NLI model.\n",
    "                       Default is RoBERTa-large fine-tuned on MNLI.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Loading NLI Judge: {model_name}\")\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name\n",
    "        ).to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Extract label mapping from model configuration\n",
    "        self.id2label = {\n",
    "            int(k): v.lower()\n",
    "            for k, v in self.model.config.id2label.items()\n",
    "        }\n",
    "\n",
    "        print(\"NLI Judge ready\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_labels(\n",
    "        self,\n",
    "        pairs: List[Tuple[str, str]],\n",
    "        batch_size: int = 16\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict NLI labels for a list of premise-hypothesis pairs.\n",
    "\n",
    "        Processes pairs in batches for efficiency. Each pair is classified\n",
    "        as \"entailment\", \"neutral\", or \"contradiction\".\n",
    "\n",
    "        Args:\n",
    "            pairs: List of (premise, hypothesis) tuples to classify.\n",
    "            batch_size: Number of pairs to process per batch.\n",
    "\n",
    "        Returns:\n",
    "            List of label strings corresponding to each input pair.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "\n",
    "        for start in range(0, len(pairs), batch_size):\n",
    "            # Extract current batch\n",
    "            batch = pairs[start:start + batch_size]\n",
    "\n",
    "            # Convert all inputs to strings\n",
    "            batch = [(to_text(p), to_text(h)) for p, h in batch]\n",
    "\n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            # Forward pass and get predictions\n",
    "            logits = self.model(**inputs).logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Convert prediction indices to label strings\n",
    "            labels.extend([self.id2label[int(p.item())] for p in preds])\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"\n",
    "        Release GPU memory by deleting model and tokenizer.\n",
    "\n",
    "        Should be called when the NLI Judge is no longer needed to free\n",
    "        up GPU memory for other models.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the NLI Judge\n",
    "# -----------------------------------------------------------------------------\n",
    "nli_judge = NLIJudge(CONFIG.nli_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gzz_yp5we01K"
   },
   "source": [
    "## 5. Candidate LLM Wrapper\n",
    "\n",
    "Model wrapper with 4-bit quantization support for memory-efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBLiHFZ5e01L",
    "outputId": "f78f6d3a-4e1c-4d54-fb59-878119b0987c"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Candidate Language Model Wrapper\n",
    "# =============================================================================\n",
    "# This class wraps HuggingFace causal language models for evaluation.\n",
    "# Key features:\n",
    "#   - Automatic 4-bit quantization for large models (via bitsandbytes)\n",
    "#   - Chat template formatting when available\n",
    "#   - Greedy decoding for deterministic outputs\n",
    "#   - Special handling for Phi-3 models (cache disabled to avoid errors)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class CandidateLLM:\n",
    "    \"\"\"\n",
    "    Wrapper for causal language models with quantization support.\n",
    "\n",
    "    Provides a unified interface for loading and generating text from\n",
    "    various HuggingFace causal language models. Supports 4-bit quantization\n",
    "    to enable running larger models on limited GPU memory.\n",
    "\n",
    "    Attributes:\n",
    "        model_name: HuggingFace model identifier.\n",
    "        tokenizer: Tokenizer instance for the model.\n",
    "        model: The loaded language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, quantize: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize and load a causal language model.\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier to load.\n",
    "            quantize: If True, apply 4-bit quantization using bitsandbytes.\n",
    "                     Required for larger models (>3B params) on consumer GPUs.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        quant_str = \" (4-bit quantized)\" if quantize else \"\"\n",
    "        print(f\"Loading model: {model_name}{quant_str}\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load tokenizer with trust_remote_code for custom models\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Ensure pad token is set (required for batch generation)\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # Configure model loading parameters\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        }\n",
    "\n",
    "        if quantize and DEVICE.type == \"cuda\":\n",
    "            # Configure 4-bit quantization using bitsandbytes\n",
    "            # NF4 (Normal Float 4) provides good quality with 4-bit weights\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,  # Further compression\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = bnb_config\n",
    "            model_kwargs[\"device_map\"] = \"auto\"\n",
    "        else:\n",
    "            # Standard loading with fp16 on GPU, fp32 on CPU\n",
    "            if DEVICE.type == \"cuda\":\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "                model_kwargs[\"device_map\"] = \"auto\"\n",
    "            else:\n",
    "                model_kwargs[\"torch_dtype\"] = torch.float32\n",
    "\n",
    "        # Load the model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(f\"Model ready: {model_name}\")\n",
    "\n",
    "    def _format_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Format prompt using the model's chat template if available.\n",
    "\n",
    "        Many instruction-tuned models expect a specific prompt format.\n",
    "        This method applies the chat template when available.\n",
    "\n",
    "        Args:\n",
    "            prompt: Raw user prompt text.\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string ready for generation.\n",
    "        \"\"\"\n",
    "        # Check if model has a chat template\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\") and \\\n",
    "           getattr(self.tokenizer, \"chat_template\", None):\n",
    "            try:\n",
    "                return self.tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "            except Exception:\n",
    "                pass  # Fall back to raw prompt\n",
    "        return prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        n: int = 1,\n",
    "        do_sample: bool = False,\n",
    "        max_new_tokens: int = 64,\n",
    "        **kwargs\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text completions for a given prompt.\n",
    "\n",
    "        Uses greedy decoding by default for deterministic outputs.\n",
    "        The do_sample parameter is ignored (always False) to avoid\n",
    "        CUDA synchronization errors with certain models.\n",
    "\n",
    "        Args:\n",
    "            prompt: Input text to complete.\n",
    "            n: Number of completions to generate.\n",
    "            do_sample: Ignored - always uses greedy decoding.\n",
    "            max_new_tokens: Maximum number of tokens to generate.\n",
    "            **kwargs: Additional arguments (currently unused).\n",
    "\n",
    "        Returns:\n",
    "            List of generated text completions (length n).\n",
    "        \"\"\"\n",
    "        # Apply chat template formatting\n",
    "        prompt = self._format_prompt(prompt)\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        # Phi-3 models have cache compatibility issues; disable cache\n",
    "        use_cache_setting = True\n",
    "        if \"Phi-3\" in self.model_name or \"phi-3\" in self.model_name.lower():\n",
    "            use_cache_setting = False\n",
    "\n",
    "        results = []\n",
    "        for _ in range(n):\n",
    "            try:\n",
    "                # Generate with greedy decoding (do_sample=False always)\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,  # Greedy decoding for stability\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=use_cache_setting,\n",
    "                )\n",
    "\n",
    "                # Decode only the generated portion (exclude input)\n",
    "                generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "                text = self.tokenizer.decode(\n",
    "                    generated_ids,\n",
    "                    skip_special_tokens=True,\n",
    "                ).strip()\n",
    "                results.append(text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Generation error: {str(e)[:50]}\")\n",
    "                results.append(\"\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"\n",
    "        Release GPU memory by deleting model and tokenizer.\n",
    "\n",
    "        Should be called after evaluation of each model to free memory\n",
    "        before loading the next model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "print(\"CandidateLLM class defined (with 4-bit quantization support)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0X2NWkqBe01M"
   },
   "source": [
    "## 6. Dataset Loading\n",
    "\n",
    "Load and preprocess the three benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944,
     "referenced_widgets": [
      "ac48a2f6146e483ab4a471142edebba9",
      "d7f0afccd28a47f0b6535473647dd254",
      "eb5fcdfa11f9438fb20bbda90bf31399",
      "d0aae32a9d3a476da833d4f0a18da115",
      "e2bc30ab440e4de4a7a724b8ff5a2579",
      "30a635138eec48349c02592221cc7b32",
      "962008fa48f445daa1da4413210ccb7b",
      "21dcc56f688a4448b9c90d290385f85d",
      "818c0340611747c1a1d552cd491d804f",
      "78c94fd225054c4ca103e2e2362f529a",
      "c087cae9c5bb4c18a0bbf8f90909ebb9",
      "ace53fbe1e0e44d8816288e1a97bda51",
      "9dee5fcd1da84e8a9736a3891749431c",
      "064b6b5776ed4aae9696a56e1b16cd12",
      "813f5ea83be74dbf9a6e5973f5376845",
      "8c6c41b3691b4326939422721b871572",
      "ec337ecbfab447a9b4ef55d6beace425",
      "7a394ebce9b74bf8acd3543c8de61586",
      "e9ffdc8010a94a648498300ada714db6",
      "e93b9673077c42c399df6a1e2e83d9d2",
      "4dd55aafec414b9893e2de96412b18f1",
      "c3f98de3cf4e49c189a50d94d12c29a7",
      "448931fd7afd4921b53e1a8c531a12cf",
      "30fff7216f6e4c9fa56efd7462eb488a",
      "53176daa1fac42ae994f6556e5cea0e2",
      "f9374e3e3853497c973af05cb55852e7",
      "75a679f0bb04497abbc425454a666502",
      "d1fc984ec18142eebbbc4238f428450f",
      "07eaae0d69e144f8b69954dc1d06326b",
      "cc35c688054242c290a057955e816aea",
      "89dfe01e57d14ccea1947d954a5f9509",
      "8cd972bd2e3d4c5a890e1803b124f4c9",
      "953fc2d6c98444c59fd91dfef2cba658",
      "dc22d07fb5de4ee5acd32049b334ace0",
      "9a468102885443dbb269d2bbe0254a6e",
      "494377e0804c42f3bef94ca1804e3019",
      "ce058ef9b1534f108a8a98aa18a49b29",
      "0e5f510888d24d1f98e4ba83c98d4bad",
      "92f133e91f854d97ae11951213540bbf",
      "0a8d00c766b945c88bed26842f50ad72",
      "e9616818b37b45f79dbeb4de38ff1d67",
      "3ab06c9f98874910b737d82307fb94d0",
      "6894ff9e40b4459ca58484c7d02a6ba4",
      "6398b2309a2a4abe9cd3bd65ba9f810d",
      "275d1ab3ac0e4b41989ce15ee92d13f3",
      "10a400eadc944a79babefbb534bbebac",
      "64e63796a3c549c6952bb584eedbac84",
      "b470c5f2bcd446eeb392deb5b75080b5",
      "5ede1329542e46e1b7b31a5e57706e7d",
      "c740e514934c44f699c2b75d1c8a6520",
      "11700098b8f94beeb943c094dc13d315",
      "2635332ae8ac4adc8743059b9b5a17d4",
      "853013a2292d4161b84ebf2980b9bb29",
      "a5711f66b7ff4835a9a6bbb12f704fee",
      "43e475124b80468d8b4e4ad15db34539",
      "b7821c75ca3b4f888109428085335879",
      "3bd71fe811074b4fa4f4ab53d2326faa",
      "11ce427d707c4ffc9e61e35738683d47",
      "c0a32b4e97aa478abee074c56fcb474c",
      "74ba8cca6634457db4d3ec9b56e19555",
      "6e2486e23a474ce6b876df09a8f8e7d2",
      "33c840d7995342aa9e91e8f6abe2b0cd",
      "aa36b6c4d2dd4521afc6abcc6b2445a5",
      "968ba421d3264487ba126337bbcbbea3",
      "c54c5c200642431e83b15735fdbea6ed",
      "b70691cab51f4ada8b73f6a1ca0cf201",
      "c8bd3a40e4454928a89bb48c82463bd8",
      "ea14a7a7ef814693b16f9ffa761538de",
      "ed46e5fc95914dc69238409e8ae68760",
      "766ebfc0d3dd47489b39a7c452f03cb2",
      "2255fb853b914944ad8d561393760d14",
      "dba0c63d729d4cfca5ff091b649e0ec9",
      "043513bb24a547d8bb960d8e74c9408e",
      "92da69d01cc64c5aaed586d297063b50",
      "2857c3aeb8b84574986156bf663602ae",
      "39f5b51064ab4920bcd2ca4b0aac067c",
      "d2b4d0fca44a43c3adce092b727527a1",
      "554960782d83495bbea06fd8f381a81b",
      "878b374c464444b582d668c71f29e733",
      "413ecd83bbbb4229a1f81bdb3ba79fab",
      "f62d4781c711499e846bc4ae1d0ad024",
      "ef56e8cdbea2459689318c3ff8e64e47",
      "decad36080bc4999a34fbb9f927d64b2",
      "71ca9df1c7884652bc583748bed02609",
      "115047a5dddc4a77b0d51980a69f40eb",
      "7ab5ea2b2b944f3798e5c22017b5712b",
      "a6c3dd144f0e45489dccb789e3a00ddf",
      "41021f20d4744fb6881ec7daed0a8b3e",
      "8d4fcff754514a37b343870642ba79d7",
      "3705f1da22a14db084d3892cb5e429fe",
      "1bc29325351b44c2aeebf60b2c90d13c",
      "a8455dd7f6c749f0a61b0493c79b41ea",
      "2fabfff3e5f84556a61814a3c9f39757",
      "148841e968504147b398ac1a5d5a92a0",
      "111639c12ec643299f6871803306187b",
      "52e9e7b1397c49b5baac1e79df938c97",
      "14c0b0cc5b04415da1ba310e77148998",
      "298169c78faa4141b6981a38fd9a5f02",
      "ada314fb2fad49c2bb91a2fe4a8d4abf",
      "5a84159d61244eaf84370c59424e2f74",
      "fd3cc506a73f428e9f3f7ddb2b9512b5",
      "0a078c8b2d2e49ab8d3a68b907ce4b03",
      "b2e2e7a90c7244ce8660d3c7dcd234c4",
      "129e8aa8d65246c9bad9f1d889a7945f",
      "66a134491a32463ead3829078a5b49e9",
      "3a44f68f2fde46cfa9ef712a9e39c611",
      "4f244b51e6be4430902fa17ac4e358e8",
      "fc60e545315e43189be3da494839f6e0",
      "fdc801c2c8c04eaca3baacdd6ebdebbd",
      "6c8353c53a3b40268ddc7f89fa930c47",
      "58e2e7a8d8bd4a729a0e05a97976f27a",
      "4ef8efd02dd9416a8a5e35ed3548e16d",
      "16f82121b3d54805a07b07ba8ffd11e6",
      "45b22ce0c25b442ab08635b2e2425b39",
      "c31dcbc19dbe4612bf29e86f34668253",
      "42cd1e0a67274686a858acb7ebe9d1ff",
      "33a9e6754e0949cd8aa038f68db2f76b",
      "c0f4d3a74c7748c393b6cf695c191c99",
      "4df625860a82472fa62a4a3c8fc43e52",
      "7c6b9e55acde47e890f084d31f6af555",
      "a229c49781cc4caf8c587902d3877081",
      "6876b28b88104bdb8a7c5274f3be64a3",
      "6242e785a6934b87a4390b954df4f767",
      "ac1de7b6ca824d1baca6699c7555b98d",
      "f3e51389aeeb430c9983deded805ec95",
      "06fd0af102564959ac46f87db841ca20",
      "b2877d0750fe4cba92c11be3af81399e",
      "425b87a760ac4486bf778fbae035f85c",
      "03b801ab3bcc423fb86572a4fe463603",
      "f35dd34a752b4f75b3b59a4778fafb11",
      "3af87201720a48ddb1f157faaa414047",
      "9f97da1cd0334d6086f7bc03312db805",
      "601077a72a044027815e70b2124715cb",
      "0a26b515d2ae413ab25e19dcb8ec7cbc",
      "513291eb783e469baffa8b883a09cb07",
      "14da155468b844caad1a5437e3d55fa5",
      "991a91b07a45473bb4780c5abde3db2f",
      "c77b08cc61d44d35a2f18eb95fca2f08",
      "3ba4bf44e13c41fd8568d368beb3e937",
      "77fe1477cd7a4bf28ebdaef2a5908000",
      "dfffe7eb36cd45868fe939100eab0459",
      "ba0508a00fa2415096f6f944b4ac1183",
      "68f0c009c89f48d29440a9c3d0a5a906",
      "d640358b9955409b876f70a519054510",
      "82063f34cd384cd59aee45bed9d122e2",
      "5c2d158ecc924242a33ad156b8419194",
      "0ccd335e1c8447f98512db22ff8ffe1f",
      "91d2c931eb13423488f84e500d7bd327",
      "eb44e3380c024c27bc7b00286cf9bbb3",
      "4d7043465a444dff9f5c694dbd9c3b9e",
      "8665a016d4f64e4288b75929a2beae46",
      "ee08a2a72c1d4e24a4924fc465688bcb",
      "413ff70fb9ed40d48e0f63d80e08689b",
      "ce58fa53045e43499516b6c48e2eb610",
      "117d99cd2e9a41f5bb97073cd8bcba60",
      "34710b2dbd6b473894072bd4cc44bf79",
      "2c96ec4a5feb4baa8b2d94751326399b",
      "cf1507866b364363be5af4b619d7faa1",
      "9c10499d8b8241c798d6a31e045758f5",
      "ebf50a710d6043ffbb7294a1904796c8",
      "15a9b673e91447aeb65b780d122c27ea",
      "3e86b9b36a8f405398c7c989a6c0a296",
      "ce6cb436465d44f89fa8934b469ae87a",
      "edbef6bdba114b71a2cfad0f814119aa",
      "3c580e054b2143869878dd78b78a0d64",
      "67e79964c2ec4633b032397231dc2b75",
      "120a63daf88144e38970d75c148aa5c2",
      "eef36fae9776485c93702e00fc1edf6e",
      "da5b09ad8a11470e85e8a6850ce5ae71",
      "027d9e3b083e461d896a68f6d32a7331",
      "1e355bad7f604c91b016a4028f869ffb",
      "d106b50c211646db9c61f86e2a55f991",
      "478973ce7581435e880dc73d3155c215",
      "667b87bb5b3044e0b402de487d64beb6",
      "3990df93a0334897a57b89531ed4ea82",
      "148aabb144ee4bf782e5ee7567da21c7",
      "7936413f6f71408f8f8bf36a448b7dae",
      "f7b77f9b526b45ba9e1ea8df451e32b3",
      "bd656b95c3314eb9884beafbc4d435c9",
      "679017dce1b24bc0ac22dd1914404302",
      "8dc926f7f731451499507818dd4f6ca3",
      "352898879f7d412680ee8602c8673828",
      "bf2f91f189194b5e9861e6bd9f646126",
      "2a576a4413334ba3bb3bc3222dfc3a56",
      "df8e72c18f5547d199577d5b5f317c7d",
      "d32a4949ed2648ad91930dd341523c7d",
      "7ec1443a28d04cfbbf3d19cd52a42822",
      "869ad374c7524f0b9d05c40c4eecb9ca",
      "3db32f0ee44f4e1b9da7a343dd63de32",
      "c059394e59764d749b9cf4ce5945798b",
      "b14a916734ff420fa1dcf42a8a395500",
      "ca43b68fce6541b4b7cbaf9bdd07da32",
      "5d94e14c4ff2416c908462ade1e7cdaf",
      "6581584dfc554b8c8966469c1da4a4d1",
      "10a512ef779b4787ac3d058171f846c2",
      "af9c3295b1854631b410001f3343219a",
      "88773a88d7e3475980bc299994e07b1f",
      "962c64aaf5ec40beb4cd8248a1f33895",
      "28294f3a7dea435db6740151fd75d9d6",
      "20d3c80b305346c8acac80b91985c0c2",
      "a23e6d44d28e4eb3a847b1bbdb352ce9",
      "1b384bf2d51f43369caf72fe0c4e12c5",
      "6f594e8c9e674128839e82f5892738f8",
      "cb440387e23444c0b9f3443cd8ba0810",
      "2676be859ee646a0853f06d15962ab55",
      "3cc368b718be4b08812d7f242276f474",
      "c42330dfa7f5495b835fd2f087eaac16",
      "9efe0a82e2074c3a9121647784335ff5",
      "5e139d15d5324a7ba084986c4c801b0c",
      "a7269e13cfdf4a66b07ced5204f52065",
      "7459405a07c44bfba02778d014281975",
      "5e20b86beae64b8889262eb1c43ff9c5",
      "0b6c11f7fa1d405088c445e2cc550104",
      "573d5a3038d84c43be6f685e6aad4432",
      "607f0cc93e2d4e65897da0b4c5222741",
      "01bc4e93271d449e86cb35556d9a2f9c",
      "3794117012464736a49f5ddf17352c14",
      "24f5bf5a7bb442ed88c78fc6b5433eab",
      "67a60c9d88db4e13a3d5b1bfe465160c",
      "854d6e1bf3ee4a0a9b7939c7543b7955",
      "6175e819c8b64511828229cf1aa865b9",
      "d3c1c7bae7584ffe8cbc256205b9de3e",
      "7cc26614a4bb4a6eba3526e9726a26fe",
      "e680fab4c3124d7c9ced780994387e25",
      "180591715ba448e19e609aef761c3be6",
      "e7a0a61b87da41329e7e953fd73abbb3",
      "994e908768d34248afc2b503c0281871",
      "bf037e79c9c14f7196693f12494ef2e7",
      "09e67f1022c1446f95236853eb8202c6",
      "04f39ad9d567403c9e5e064f4438e3ce",
      "64691e33915140e18061d6383f25c8d1",
      "52990620102d4c7e8ad4ed779b5c5c68",
      "817735fd7c52437298ca723af1381d08",
      "11188f32ad674f77bebbd611c2440849",
      "ff64fe5b92b54c8f94b5e2707b7974ba",
      "2881113ae73f46a682bc09f92e3a6042",
      "38ef976751f74aa2a890578e6412392a",
      "3c93c156d87743779f498c76ec44efb9",
      "628385fd884d4fe99eb360c335b4437d",
      "233a625fb19c45509ec4a292ac9174ab",
      "ae9ef363b5814dc2b03baa7a8fe521c4",
      "8981330ecb3d4b7080d3044fbbc80c3a",
      "578b478d42664dcea9944d6c1fc21b63",
      "5969f294b08847c9819d0147ad808fd4",
      "d9ec8ddcd1fa4d27bc215835093194a9",
      "7efade6fa93f40689ecb8183408787a7",
      "923f4e819dac4144b7100be00c615d80",
      "f7781068f32242768f5ddffe7655d94f",
      "61ebb4e4e0bf44d19a9d1fec680b9761",
      "d02a1e6677c14495ba3ac9624bac9db3",
      "2aabff74f9af4949a71fd47b2e3295dd",
      "7a654147c5ca4a6ba18d0621c9605491",
      "f9acfc2218714d83b2f219da385095d5"
     ]
    },
    "id": "lljSDyRve01N",
    "outputId": "2dcfda92-d12d-4c57-a7a5-939df6c74c47"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset Loading and Preprocessing\n",
    "# =============================================================================\n",
    "# This module handles loading the three benchmark datasets:\n",
    "#   1. EntailmentBank: Science QA with reasoning chains\n",
    "#   2. RuleTaker: Synthetic logical reasoning problems\n",
    "#   3. WebNLG: Knowledge graph to text generation\n",
    "#\n",
    "# RuleTaker filtering selects harder examples with more rules to better\n",
    "# discriminate between models of different capabilities.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def try_load(\n",
    "    name: str,\n",
    "    config: Optional[str] = None,\n",
    "    split: Optional[str] = None\n",
    ") -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Attempt to load a HuggingFace dataset with error handling.\n",
    "\n",
    "    Provides a safe wrapper around load_dataset that catches exceptions\n",
    "    and returns None on failure, allowing fallback loading strategies.\n",
    "\n",
    "    Args:\n",
    "        name: HuggingFace dataset identifier (e.g., \"suzakuteam/entailment_bank\").\n",
    "        config: Optional dataset configuration name.\n",
    "        split: Optional dataset split (e.g., \"train\", \"test\").\n",
    "\n",
    "    Returns:\n",
    "        Loaded dataset if successful, None if loading failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if config is None and split is None:\n",
    "            return load_dataset(name, trust_remote_code=True)\n",
    "        if config is None:\n",
    "            return load_dataset(name, split=split, trust_remote_code=True)\n",
    "        return load_dataset(name, config, split=split, trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[LOAD FAILED] {name} | config={config} | split={split}\")\n",
    "        print(f\"  Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def count_rules_in_context(context: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of rules/facts in a RuleTaker context string.\n",
    "\n",
    "    Uses heuristics to estimate complexity:\n",
    "      - Counts sentence-ending punctuation followed by whitespace\n",
    "      - Counts \"If\" statements which indicate conditional rules\n",
    "\n",
    "    Args:\n",
    "        context: The context string from a RuleTaker example.\n",
    "\n",
    "    Returns:\n",
    "        Estimated number of rules/facts in the context.\n",
    "    \"\"\"\n",
    "    if not context:\n",
    "        return 0\n",
    "\n",
    "    # Count sentences (periods, exclamation marks, question marks)\n",
    "    sentences = len(re.findall(r'[.!?]+\\s+|[.!?]+$', context))\n",
    "\n",
    "    # Count explicit conditional rules (\"If X then Y\")\n",
    "    rules = len(re.findall(r'\\bIf\\s+', context, re.IGNORECASE))\n",
    "\n",
    "    return max(sentences, rules)\n",
    "\n",
    "\n",
    "def filter_hard_ruletaker(\n",
    "    dataset: Any,\n",
    "    min_rules: int = 5,\n",
    "    max_samples: int = 100\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Filter RuleTaker dataset for harder examples with more rules.\n",
    "\n",
    "    Examples with more rules require longer reasoning chains, which\n",
    "    better discriminates between models of different capabilities.\n",
    "\n",
    "    Args:\n",
    "        dataset: Full RuleTaker dataset to filter.\n",
    "        min_rules: Minimum number of rules required (default: 5).\n",
    "        max_samples: Maximum number of samples to return (default: 100).\n",
    "\n",
    "    Returns:\n",
    "        Filtered dataset containing only harder examples.\n",
    "    \"\"\"\n",
    "    hard_indices = []\n",
    "\n",
    "    # Scan dataset for examples meeting the difficulty threshold\n",
    "    for i, ex in enumerate(dataset):\n",
    "        ctx = to_text(ex.get(\"context\", \"\"))\n",
    "        num_rules = count_rules_in_context(ctx)\n",
    "\n",
    "        if num_rules >= min_rules:\n",
    "            hard_indices.append(i)\n",
    "\n",
    "        # Early stopping if we have enough candidates\n",
    "        if len(hard_indices) >= max_samples * 2:\n",
    "            break\n",
    "\n",
    "    # Handle case where no examples meet the threshold\n",
    "    if len(hard_indices) == 0:\n",
    "        print(f\"Warning: No examples with >= {min_rules} rules found\")\n",
    "        print(\"  Falling back to sequential selection\")\n",
    "        return dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    # Randomly sample from qualified examples for variety\n",
    "    random.shuffle(hard_indices)\n",
    "    selected = hard_indices[:max_samples]\n",
    "\n",
    "    print(f\"Filtered RuleTaker: {len(selected)} hard examples \"\n",
    "          f\"(>= {min_rules} rules each)\")\n",
    "\n",
    "    return dataset.select(selected)\n",
    "\n",
    "\n",
    "def load_datasets(cfg: LACEConfig) -> Tuple[Any, Any, Any]:\n",
    "    \"\"\"\n",
    "    Load all three benchmark datasets for LACE evaluation.\n",
    "\n",
    "    Loads and preprocesses:\n",
    "      - EntailmentBank: Science QA with explanation chains\n",
    "      - RuleTaker: Logical reasoning (filtered for harder examples)\n",
    "      - WebNLG: Knowledge graph verbalization\n",
    "\n",
    "    Args:\n",
    "        cfg: LACEConfig instance containing dataset size parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (entailmentbank_subset, ruletaker_subset, webnlg_subset).\n",
    "        Any subset may be None if loading failed.\n",
    "    \"\"\"\n",
    "    subset_eb = subset_rt = subset_wn = None\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load EntailmentBank\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Loading EntailmentBank...\")\n",
    "    eb = (\n",
    "        try_load(\"suzakuteam/entailment_bank\", config=\"default\", split=\"train\")\n",
    "        or try_load(\"suzakuteam/entailment_bank\", config=None, split=\"train\")\n",
    "    )\n",
    "    if eb is not None:\n",
    "        subset_eb = eb.select(range(min(cfg.num_samples, len(eb))))\n",
    "        print(f\"  EntailmentBank: {len(subset_eb)} samples loaded\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load RuleTaker (with difficulty filtering)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Loading RuleTaker...\")\n",
    "    rt = try_load(\"tasksource/ruletaker\", config=None, split=\"test\")\n",
    "    if rt is not None:\n",
    "        subset_rt = filter_hard_ruletaker(\n",
    "            rt,\n",
    "            min_rules=cfg.min_ruletaker_rules,\n",
    "            max_samples=cfg.num_ruletaker_samples\n",
    "        )\n",
    "        print(f\"  RuleTaker: {len(subset_rt)} hard samples loaded\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load WebNLG\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Loading WebNLG...\")\n",
    "    wn = (\n",
    "        try_load(\"GEM/web_nlg\", config=\"en\", split=\"test\")\n",
    "        or try_load(\"GEM/web_nlg\", config=None, split=\"test\")\n",
    "    )\n",
    "    if wn is not None:\n",
    "        subset_wn = wn.select(range(min(cfg.num_samples, len(wn))))\n",
    "        print(f\"  WebNLG: {len(subset_wn)} samples loaded\")\n",
    "\n",
    "    return subset_eb, subset_rt, subset_wn\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset Field Extraction Helpers\n",
    "# =============================================================================\n",
    "# These functions extract specific fields from dataset examples,\n",
    "# handling variations in field names across different dataset versions.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def get_eb_question(ex: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract the question from an EntailmentBank example.\n",
    "\n",
    "    Args:\n",
    "        ex: Dictionary containing the dataset example.\n",
    "\n",
    "    Returns:\n",
    "        Question string, or empty string if not found.\n",
    "    \"\"\"\n",
    "    for key in [\"question\", \"query\", \"prompt\", \"input\"]:\n",
    "        if key in ex and to_text(ex[key]).strip():\n",
    "            return to_text(ex[key]).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_eb_gold(ex: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract the gold answer from an EntailmentBank example.\n",
    "\n",
    "    Args:\n",
    "        ex: Dictionary containing the dataset example.\n",
    "\n",
    "    Returns:\n",
    "        Gold answer string, or empty string if not found.\n",
    "    \"\"\"\n",
    "    for key in [\"answer\", \"gold_answer\", \"output\", \"label\", \"target\"]:\n",
    "        if key in ex and to_text(ex[key]).strip():\n",
    "            return to_text(ex[key]).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_eb_context(ex: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract the context/premises from an EntailmentBank example.\n",
    "\n",
    "    Args:\n",
    "        ex: Dictionary containing the dataset example.\n",
    "\n",
    "    Returns:\n",
    "        Context string, or empty string if not found.\n",
    "    \"\"\"\n",
    "    for key in [\"context\", \"premises\", \"premise\", \"facts\", \"supporting_facts\"]:\n",
    "        if key not in ex:\n",
    "            continue\n",
    "        v = ex.get(key)\n",
    "        if isinstance(v, list):\n",
    "            txt = \"\\n\".join(to_text(x) for x in v).strip()\n",
    "        else:\n",
    "            txt = to_text(v).strip()\n",
    "        if txt:\n",
    "            return txt\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_rt_gold(ex: Dict[str, Any]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Extract the gold label from a RuleTaker example.\n",
    "\n",
    "    Args:\n",
    "        ex: Dictionary containing the dataset example.\n",
    "\n",
    "    Returns:\n",
    "        Gold label value, or None if not found.\n",
    "    \"\"\"\n",
    "    for key in [\"label\", \"answer\", \"gold\", \"target\"]:\n",
    "        if key in ex and ex[key] is not None:\n",
    "            return ex[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_wn_triples(ex: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract knowledge graph triples from a WebNLG example.\n",
    "\n",
    "    Handles the GEM/web_nlg format where triples may be stored\n",
    "    in various fields as lists or newline-separated strings.\n",
    "\n",
    "    Args:\n",
    "        ex: Dictionary containing the dataset example.\n",
    "\n",
    "    Returns:\n",
    "        List of triple strings in \"subject | predicate | object\" format.\n",
    "    \"\"\"\n",
    "    for key in [\"input\", \"triples\", \"mr\", \"source\"]:\n",
    "        if key not in ex:\n",
    "            continue\n",
    "        v = ex[key]\n",
    "        if isinstance(v, list):\n",
    "            return [to_text(x).strip() for x in v if to_text(x).strip()]\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return [s.strip() for s in v.splitlines() if s.strip()]\n",
    "    return []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Load Datasets\n",
    "# =============================================================================\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Datasets\")\n",
    "print(\"=\" * 60)\n",
    "subset_eb, subset_rt, subset_wn = load_datasets(CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ7OICI1e01O"
   },
   "source": [
    "## 7. Scoring Functions\n",
    "\n",
    "Implementation of the six LACE metrics and the final LCS computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bC-kkdK-e01O",
    "outputId": "d5301021-d063-4f9d-a21f-ee46f2c0f694"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Metric Scoring Functions\n",
    "# =============================================================================\n",
    "# This module implements the six LACE metrics:\n",
    "#\n",
    "# EntailmentBank metrics:\n",
    "#   - EC (Entailment Classification): Does output entail the gold answer?\n",
    "#   - CR (Contradiction Rate): Does output contradict the context?\n",
    "#\n",
    "# RuleTaker metrics:\n",
    "#   - RS_F (Relational Structure - Formal): Logical reasoning accuracy\n",
    "#   - AC (Answer Consistency): Self-consistency across prompt variations\n",
    "#   - KAS (Knowledge Attribute Scoring): Sensitivity to counterfactuals\n",
    "#\n",
    "# WebNLG metric:\n",
    "#   - RS_KG (Relational Structure - KG): Knowledge graph verbalization\n",
    "#\n",
    "# Final score:\n",
    "#   - LCS (Logic Consistency Score): Weighted combination of all metrics\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def score_entailmentbank(\n",
    "    llm: CandidateLLM,\n",
    "    subset: Any,\n",
    "    nli_judge: NLIJudge,\n",
    "    config: LACEConfig\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute EC and CR metrics on EntailmentBank dataset.\n",
    "\n",
    "    EC (Entailment Classification): Measures whether the model's response\n",
    "    semantically entails the gold answer.\n",
    "\n",
    "    CR (Contradiction Rate): Measures how often the model's response\n",
    "    contradicts the given context/premises.\n",
    "\n",
    "    Args:\n",
    "        llm: The candidate language model to evaluate.\n",
    "        subset: EntailmentBank dataset subset.\n",
    "        nli_judge: NLI model for entailment classification.\n",
    "        config: Configuration containing generation parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (EC_score, CR_score) as floats in range [0, 1].\n",
    "    \"\"\"\n",
    "    if subset is None or len(subset) == 0 or nli_judge is None:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    ec_vals = []  # Entailment scores\n",
    "    cr_vals = []  # Contradiction scores\n",
    "\n",
    "    for ex in tqdm(subset, desc=\"EntailmentBank\", leave=False):\n",
    "        question = get_eb_question(ex)\n",
    "        gold_answer = get_eb_gold(ex)\n",
    "        context = get_eb_context(ex) or question\n",
    "\n",
    "        if not question:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Generate model response\n",
    "            response = llm.generate(\n",
    "                f\"Question: {question}\\nAnswer:\",\n",
    "                n=1,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=config.max_new_tokens\n",
    "            )[0]\n",
    "\n",
    "            if not response.strip():\n",
    "                continue\n",
    "\n",
    "            # EC: Check if response entails gold answer\n",
    "            ec_label = nli_judge.predict_labels([(response, gold_answer)])[0]\n",
    "            ec_vals.append(1.0 if ec_label == \"entailment\" else 0.0)\n",
    "\n",
    "            # CR: Check if context contradicts response\n",
    "            cr_label = nli_judge.predict_labels([(context, response)])[0]\n",
    "            cr_vals.append(1.0 if cr_label == \"contradiction\" else 0.0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      EntailmentBank error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Compute mean scores\n",
    "    ec_score = float(np.mean(ec_vals)) if ec_vals else 0.0\n",
    "    cr_score = float(np.mean(cr_vals)) if cr_vals else 0.0\n",
    "\n",
    "    return ec_score, cr_score\n",
    "\n",
    "\n",
    "def score_ruletaker(\n",
    "    llm: CandidateLLM,\n",
    "    subset: Any,\n",
    "    config: LACEConfig\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute RS_F, AC, and KAS metrics on RuleTaker dataset.\n",
    "\n",
    "    RS_F (Relational Structure - Formal): Accuracy of logical reasoning.\n",
    "    Measures whether the model correctly applies logical rules.\n",
    "\n",
    "    AC (Answer Consistency): Self-consistency across prompt variations.\n",
    "    Higher scores indicate the model gives consistent answers.\n",
    "\n",
    "    KAS (Knowledge Attribute Scoring): Sensitivity to counterfactuals.\n",
    "    Measures whether the model changes its answer when premises are negated.\n",
    "\n",
    "    Args:\n",
    "        llm: The candidate language model to evaluate.\n",
    "        subset: RuleTaker dataset subset.\n",
    "        config: Configuration containing generation parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (RS_F_score, AC_score, KAS_score) as floats in range [0, 1].\n",
    "    \"\"\"\n",
    "    if subset is None or len(subset) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    rsf_vals = []  # Formal reasoning accuracy\n",
    "    ac_vals = []   # Answer consistency\n",
    "    kas_vals = []  # Counterfactual sensitivity\n",
    "\n",
    "    # Instruction for boolean question answering\n",
    "    instruction = \"Reply with exactly: Answer: True OR Answer: False\\n\"\n",
    "\n",
    "    for ex in tqdm(subset, desc=\"RuleTaker\", leave=False):\n",
    "        context = to_text(ex.get(\"context\"))\n",
    "        question = to_text(ex.get(\"question\"))\n",
    "        expected = normalize_ruletaker_label(get_rt_gold(ex))\n",
    "\n",
    "        prompt = f\"{context}\\nQuestion: {question}\\n{instruction}\"\n",
    "\n",
    "        try:\n",
    "            # -----------------------------------------------------------------\n",
    "            # RS_F: Greedy generation for accuracy\n",
    "            # -----------------------------------------------------------------\n",
    "            response = llm.generate(\n",
    "                prompt,\n",
    "                n=1,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=32\n",
    "            )[0]\n",
    "            predicted = extract_final_boolean(response)\n",
    "\n",
    "            # Score: 1 if prediction matches expected, 0 otherwise\n",
    "            rsf_score = 1.0 if (predicted is not None and\n",
    "                                predicted == expected) else 0.0\n",
    "            rsf_vals.append(rsf_score)\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # KAS: Counterfactual sensitivity\n",
    "            # -----------------------------------------------------------------\n",
    "            # Create negated version of context\n",
    "            counterfactual_prompt = (\n",
    "                f\"{perturb_context(context)}\\n\"\n",
    "                f\"Question: {question}\\n{instruction}\"\n",
    "            )\n",
    "            cf_response = llm.generate(\n",
    "                counterfactual_prompt,\n",
    "                n=1,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=32\n",
    "            )[0]\n",
    "            cf_predicted = extract_final_boolean(cf_response)\n",
    "\n",
    "            # Score: 1 if answer changed (model is sensitive), 0 otherwise\n",
    "            kas_score = 1.0 if (predicted is not None and\n",
    "                                cf_predicted is not None and\n",
    "                                predicted != cf_predicted) else 0.0\n",
    "            kas_vals.append(kas_score)\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # AC: Answer consistency across prompt variations\n",
    "            # -----------------------------------------------------------------\n",
    "            # Use different prompt phrasings to test consistency\n",
    "            prompt_variations = [\n",
    "                prompt,\n",
    "                f\"Context: {context}\\nQ: {question}\\n{instruction}\",\n",
    "                f\"{context}\\n\\nQuestion: {question}\\n{instruction}\",\n",
    "            ]\n",
    "\n",
    "            votes = []\n",
    "            for prompt_var in prompt_variations[:config.num_consistency_generations]:\n",
    "                try:\n",
    "                    var_response = llm.generate(\n",
    "                        prompt_var,\n",
    "                        n=1,\n",
    "                        do_sample=False,\n",
    "                        max_new_tokens=32\n",
    "                    )[0]\n",
    "                    vote = extract_final_boolean(var_response)\n",
    "                    if vote is not None:\n",
    "                        votes.append(vote)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # AC score: fraction of votes matching the majority\n",
    "            if votes:\n",
    "                vote_counts = Counter(votes)\n",
    "                ac_score = max(vote_counts.values()) / len(votes)\n",
    "            else:\n",
    "                ac_score = 0.0\n",
    "            ac_vals.append(ac_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      RuleTaker error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Compute mean scores\n",
    "    rsf_score = float(np.mean(rsf_vals)) if rsf_vals else 0.0\n",
    "    ac_score = float(np.mean(ac_vals)) if ac_vals else 0.0\n",
    "    kas_score = float(np.mean(kas_vals)) if kas_vals else 0.0\n",
    "\n",
    "    return rsf_score, ac_score, kas_score\n",
    "\n",
    "\n",
    "def score_webnlg(\n",
    "    llm: CandidateLLM,\n",
    "    subset: Any,\n",
    "    nli_judge: NLIJudge\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute RS_KG metric on WebNLG dataset.\n",
    "\n",
    "    RS_KG (Relational Structure - Knowledge Graph): Measures how well\n",
    "    the model verbalizes knowledge graph triples. Uses NLI to check\n",
    "    if the generated sentence entails each input triple.\n",
    "\n",
    "    Args:\n",
    "        llm: The candidate language model to evaluate.\n",
    "        subset: WebNLG dataset subset.\n",
    "        nli_judge: NLI model for entailment checking.\n",
    "\n",
    "    Returns:\n",
    "        RS_KG score as float in range [0, 1].\n",
    "    \"\"\"\n",
    "    if subset is None or len(subset) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    rs_vals = []\n",
    "\n",
    "    def humanize_relation(rel: str) -> str:\n",
    "        \"\"\"Convert camelCase or snake_case relation to readable text.\"\"\"\n",
    "        rel = (rel or \"\").replace(\"_\", \" \").strip()\n",
    "        # Insert space before capitals (camelCase to words)\n",
    "        rel = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", rel)\n",
    "        return re.sub(r\"\\s+\", \" \", rel).strip()\n",
    "\n",
    "    def triple_to_hypothesis(triple: str) -> str:\n",
    "        \"\"\"Convert a triple string to a natural language hypothesis.\"\"\"\n",
    "        parts = [p.strip() for p in to_text(triple).split(\"|\")]\n",
    "        if len(parts) >= 3:\n",
    "            subject = parts[0]\n",
    "            predicate = humanize_relation(parts[1])\n",
    "            obj = parts[2]\n",
    "            return f\"{subject} {predicate} {obj}.\"\n",
    "        return to_text(triple).strip()\n",
    "\n",
    "    for ex in tqdm(subset, desc=\"WebNLG\", leave=False):\n",
    "        triples = get_wn_triples(ex)\n",
    "        if not triples:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Create prompt for triple verbalization\n",
    "            prompt = (\n",
    "                \"Convert these facts into ONE sentence.\\n\"\n",
    "                f\"{chr(10).join(triples)}\\n\"\n",
    "                \"Sentence:\"\n",
    "            )\n",
    "\n",
    "            # Generate verbalization\n",
    "            output = llm.generate(\n",
    "                prompt,\n",
    "                n=1,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=100\n",
    "            )[0]\n",
    "\n",
    "            if not output.strip():\n",
    "                rs_vals.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Check if output entails each triple using NLI\n",
    "            if nli_judge:\n",
    "                hypotheses = [triple_to_hypothesis(t) for t in triples]\n",
    "                pairs = [(output, h) for h in hypotheses if h]\n",
    "\n",
    "                if pairs:\n",
    "                    labels = nli_judge.predict_labels(pairs)\n",
    "                    # RS_KG: fraction of triples entailed by output\n",
    "                    entailed_count = sum(1 for l in labels if l == \"entailment\")\n",
    "                    rs_kg = entailed_count / len(pairs)\n",
    "                else:\n",
    "                    rs_kg = 0.0\n",
    "            else:\n",
    "                # Fallback: lexical overlap check\n",
    "                output_normalized = normalize_entity(output)\n",
    "                hits = sum(\n",
    "                    1 for t in triples\n",
    "                    if len(t.split(\"|\")) >= 3 and\n",
    "                    normalize_entity(t.split(\"|\")[0]) in output_normalized and\n",
    "                    normalize_entity(t.split(\"|\")[2]) in output_normalized\n",
    "                )\n",
    "                rs_kg = hits / max(1, len(triples))\n",
    "\n",
    "            rs_vals.append(rs_kg)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      WebNLG error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return float(np.mean(rs_vals)) if rs_vals else 0.0\n",
    "\n",
    "\n",
    "def compute_lcs(\n",
    "    ec: float,\n",
    "    cr: float,\n",
    "    rs_f: float,\n",
    "    rs_kg: float,\n",
    "    kas: float,\n",
    "    ac: float,\n",
    "    weights: Optional[Dict[str, float]] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the Logic Consistency Score (LCS) from individual metrics.\n",
    "\n",
    "    LCS is a weighted combination of all six metrics, designed to provide\n",
    "    a single summary score for model logical consistency.\n",
    "\n",
    "    Note: CR (contradiction rate) is inverted since lower is better.\n",
    "\n",
    "    Args:\n",
    "        ec: Entailment Classification score [0, 1].\n",
    "        cr: Contradiction Rate [0, 1] (will be inverted to 1-cr).\n",
    "        rs_f: Relational Structure - Formal score [0, 1].\n",
    "        rs_kg: Relational Structure - KG score [0, 1].\n",
    "        kas: Knowledge Attribute Scoring [0, 1].\n",
    "        ac: Answer Consistency score [0, 1].\n",
    "        weights: Optional dictionary mapping metric names to weights.\n",
    "                 Keys: 'ec', 'cr', 'ac', 'rs_f', 'rs_kg', 'kas'.\n",
    "                 If None, uses equal weights.\n",
    "\n",
    "    Returns:\n",
    "        LCS score as float in range [0, 1].\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        # Equal weights (original formula)\n",
    "        return float((ec + (1.0 - cr) + rs_f + rs_kg + kas + ac) / 6.0)\n",
    "\n",
    "    # Weighted average computation\n",
    "    components = {\n",
    "        \"ec\": ec,\n",
    "        \"cr\": 1.0 - cr,  # Invert CR (lower contradiction is better)\n",
    "        \"rs_f\": rs_f,\n",
    "        \"rs_kg\": rs_kg,\n",
    "        \"kas\": kas,\n",
    "        \"ac\": ac,\n",
    "    }\n",
    "\n",
    "    total_weight = sum(weights.values())\n",
    "    weighted_sum = sum(\n",
    "        components.get(key, 0) * weight\n",
    "        for key, weight in weights.items()\n",
    "    )\n",
    "\n",
    "    return float(weighted_sum / total_weight) if total_weight > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"Scoring functions defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmuT6Hgxe01P"
   },
   "source": [
    "## 8. Run Evaluation\n",
    "\n",
    "Execute the main evaluation loop across all models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6230b8cf725d4bb09c72c59113759f6f",
      "2e9f8e4d827840feb43af449327f3872",
      "1ac23046156e4fcb83d673469e4e3bda",
      "d6ae723d3bc84a3d91c4d7369212faf5",
      "8b0b11a3c4c248b78ccbda453dbb84ff",
      "673c3f36917a46b580e382c895dcaecf",
      "7970fcbff2fe4f7792f398a55b92bd48",
      "311d196d394a47f793dd5f44d8152e1f",
      "b28749ffe3e54c65b70fb735922c72f2",
      "efcd45cc98e34f6282aba4582c3f2849",
      "499ed4a540c24ceeaf94050a4b9317cf",
      "bbefee4a02f84994a2254b0ad4e06029",
      "8b1b2c4f7c9f4e868c3141a9dcd6bbbb",
      "f2b008230a544989812834d5896e2cf4",
      "0d754ee021164729b2bfd2ec8b07f741",
      "93c6fb809262432c9c7519128f1f6d69",
      "b67c1e747ec6403e8fd71932c83e3224",
      "7d7b2e8149994da58fd130939a1353d6",
      "cf2b26b58e2b4f8e9ab5e53009cf3524",
      "30d3069d470a40ea88bdf356d9a29ecc",
      "c69b61acfdd54bed89426d1bf1b28b03",
      "1aada447656043d1b47ffc7b13359abf",
      "d25f31c6d88b4166957ffea67e596f4f",
      "5fa8ca59c8874cfca156edb0d53354c7",
      "48d1fee0950d427fbd2311d6b32becca",
      "1ab00ef1d96c4b1685ec4dacd57f9ffc",
      "68c5848350af44df9c7b9c6b59efbc4e",
      "f5034ddd8e2d4e5388b9bc7978e53d53",
      "ea0c722357514664881cab2072b3be48",
      "f6077c0dd34746909ed4b36152d31e7d",
      "d396a0ec5947471fba2a69d7e485266a",
      "4bc3ab8b32da494089c71df483b3d84d",
      "101b2f46223c4f1d9509e964f8c69150",
      "4b37ec98bb2b42528e770dde160a6077",
      "ff55262f3fd84eae8d3125138723f695",
      "1101e1168ea447488fa10186dcaf9c9a",
      "000f8af1ff7742ca86a0f53f8b2cc4af",
      "c693608f227743a8910109d311499651",
      "c099aa1be0ce4aafb7a3e237cc8ad45f",
      "956067d9bee34ee5901bbb645ad2960c",
      "d9738fdc11584c5d88b5296a39146b94",
      "f1aece7d7f184c3f90d0a8601728773b",
      "fe9d0ce8838c4c538e5ced08c1993cd3",
      "ee39b30b6f004aa98b3a50822c2d7d37",
      "f335e9656f9948deb1e88f8267a3af52",
      "40bf44369ac44e5c938fe53408a11e39",
      "806998b9383447c788b16fe2a79422eb",
      "bbe3d83ac25b4262ab615ff84bb64d9e",
      "41d16c0d922840a4ba46d4ad1110cbbd",
      "703a65f46d664fbd8baa34fec75bb232",
      "26b5257ba276422a970185256ecc0807",
      "ced271e8806a40a0850590647d92af65",
      "1f1bc740468e460ab01a9cf0562466f6",
      "7b983c88bdd14f1a8aa8532979b7393f",
      "c50d65c0d5d348eba88cf3944b47cafe",
      "1cd81331b6914f4f89e43c9f601bcb5d",
      "5f45b424281c43c3af595bfc7d432585",
      "04cca1b48ab44bf68109ad1d27ce1b8e",
      "2c33ec14f8a74dd1acb93aa569d0d453",
      "2c50fb30ad844f2cbeb902180d68c898",
      "c1e5776f36654572aef551b726d7bd43",
      "f855a31020ae489f8715d23a3aa4c9f6",
      "08fe5e76b6174339954fa27fb45c8f10",
      "a11dde2dedaf48b6b555eb09573c93be",
      "e7b912a0502c48278a91e4f371309a06",
      "f7687599392c471e85f62b1b3c48a73d",
      "a781c78536844a2ea8cc828257449c13",
      "877478ee26074373b659cd5676c0c854",
      "2ea3753540f24412afcc4b24475a4052",
      "00821cc015474a999109a73a84344489",
      "7c60f0f9b456459380443ff7baa60c92",
      "39ea223ade1d4ba393fe6a0335b625dc",
      "5ffd2a38065543a2a75ecb9338daee7e",
      "20623781d4e14697b66beeb57d9d64ff",
      "6161772078fe425eac5f74d5da6954f4",
      "535929a0994e437486fcbd5a70e7ff70",
      "de0a5f9e3c0b432d9f0ba259b8736f4d",
      "1b50500900a54525adfc408dfb9bccd2",
      "bdb206b816c3486ca36bd1ab98400e8d",
      "a89df561405b42468b39be444b8b1242",
      "0c6d7f103da04ba2adbe242c7503758c",
      "7916aca7053946108f5bbdbcd1653673",
      "bce6bc7d94d94dd3b520be6e87c68661",
      "4964edacb3f5474f8a1d46d1e99aba21",
      "e9a32eb688bc4c61aa370b7178416006",
      "8de11a47798b4e29b6209b99a20ba35c",
      "357b345ec64849ccb1efc028d035eaf4",
      "de3d69e089fd47169246cff4b443cfa7",
      "681dfd247c3040289cd70108e74111eb",
      "4863d47ec9a64e60b535f50679836907",
      "b87e8697f10b4e00ae623dbb656e2220",
      "3ae82f8267434fe5bb6cfd69d7eb2921",
      "922a6bc070854d4da08a2a778bb7f6b6",
      "5e47653ada83411faf85102602722581",
      "1e362734e1ee4cf29cc8c90944977301",
      "c4fdc4b1549043a5ab8aa5f8efd17084",
      "316616e660dd4b75a7d604b32c1ac3a7",
      "a770ebad152c4c2781d60aec7ebb1d5b",
      "38a015652dab42aab1b546156e3e401c",
      "5e69278d71654566b696be8ce6d86f87",
      "175caa35406e4bf589eed31a2ef61b81",
      "edf7ff6407094f3b81d2b23511cf4a65",
      "a175a1ef78314c8f86d7b0ca67fc03de",
      "fa53c2865c984502a3216a31eb22b310",
      "c22727ca394c407287535be1aeb125e4",
      "d6e58fee741141219da89dff7c012895",
      "a6758c2270fd4fdc8ec1cac72443763f",
      "1d8c2b7b37784e508d6d0985a4b1ec3c",
      "906f8f3fda9847c2a79b3499c56ec40f",
      "fc028799bbd347d2b9cf61e740845d7d",
      "e827ebb70e1e487d8ce68acf981902de",
      "c9efb6009de3417ba8ed5dd5c1cc1114",
      "3353a2e6dfc740c3861d357b2cb232f5",
      "2e6b3fc9ca2643b2894ee34b19b2fb49",
      "9bbf57f8c79541f68baa0dde7fb36bff",
      "acd078c544c2442fad354a44e1da83c9",
      "d5639a130fea42febaf53f1db33ca12c",
      "35093c6a7e8c4ce9955048e67913c894",
      "711002f8571d429c9ce66cd252e5dd8e",
      "a41aebf55f784d00a1b12c64de854672",
      "ff5888ee4f434725bc5edef56fe5dbb3",
      "fffb17d48b594a8f90d5ec27545fc28b",
      "67056703692540d6bf13553e02d745b1",
      "2d7b0810bdd44fdc9983534157654ad9",
      "d50ac2e672064396a894290ca068b36e",
      "62b7ab765e984a5cb56b2e23173b394b",
      "68cee6ec8d994046be0b66b0af9a05ef",
      "677def39d9b94dec9af3a1563bd57b51",
      "826a16b19e854e27b15faee0cd2adc6e",
      "e71eaf1ead924bc2bf0128b33bc6e880",
      "215c640be3414019a840ca97e97bcc3b",
      "b903830abdf0447b9e4bc8855a21a384",
      "b4c14b7ab11f42b6a4a5863d3bf0ee2e",
      "8b7a37474e0f48ea9a72d65412ee5846",
      "b81de0108c384d19b2f84444d9adabda",
      "09fdd60d22c244e88aa239b1f8dc4973",
      "718ee980b1b645f5b75224f58ccaa823",
      "65881ea7a5b14c1ebe925e8bda842d24",
      "19f49ffed4224ab883229479433b2c1c",
      "c7aa4adc892e48f2805dce46335c0884",
      "5be1fed1b6994734af2f50cc11880fc4",
      "59ef330b1d8c4d32b9e6166d180c0fea",
      "0265b74324784c77b9e94bdc326b05b4",
      "8464f89d53c5408da67152c2ea899ec0",
      "c4f555064e624d05ab26e292ea6bab4a",
      "658806c0b2a44660995b6a9dc0265cb1",
      "c3839ca301db4e4cbec0f7d3e6371ff5",
      "bd266d2d58dd4aa0a821b3a9567e7437",
      "2abee56daaa144059f66f1d30507750c",
      "31cf3cd0370a4f79a9baf5b16585d59d",
      "e2f55be65ecd4748b1b283bdb2ad4333",
      "0034576c5d374671980b7a4aa799987e",
      "81a8499dfa3f42e699e5a05fc7151e27",
      "ec5e56909bed46deb9f2e73a6cacb534",
      "7e93cda87931479281a706bb65a239c6",
      "ac720e4bdb0a43e1ba625c3dd96dbc51",
      "83c63b41c792460b829a5d3c751cd701",
      "f97d6c239731406785af7c038f0de835",
      "000240a1c5604e029027a402924c05cf",
      "ce65779f03e443e4b3eae6b3ab8c1bc6",
      "79fdf2bfb12b4ffd8131cd403fdea198",
      "993f4a8f2ac64bfb8fe5b94df16c1930",
      "27acb2c40bd141a189ddde8678d4ee46",
      "7542a86579824e698348ea6b74af051b",
      "85106a5545df465ca04a205f8a83d3fc",
      "0ad7dc7e4f90429895d7797c083af144",
      "53cea28a1413446c9c5ff9a1650685d1",
      "8d0ce600bc3d4cb487e575cfc889c2a8",
      "ef428c8f16b44babb084b45ae5f801b7",
      "5b84b83478674f96a3caca0624b5c5cc",
      "964fa64c1f694b1da99e974885b2177c",
      "ca01f7df1a564d85b95d99c48ed4a84d",
      "927027a8144747e1885d22eaf4605c91",
      "6ec1d3196eec44e18990b63ec8646ccf",
      "c8e6e565a4a04fb7bd600f669bd59822",
      "73d4b4f6ec0a475dbe60a5dacea26aa1",
      "7e4f98ae93ea4c7baa93977133181ee2",
      "18d7b778035d4afe808e37cfd2c5222f",
      "9331f307561944c08178a339e9cb9d71",
      "359216bbd03e452997c308d74ab84663",
      "e225f900f42441fab12e5b6a410bd18f",
      "cc6bb5606cc749a486f79cccb7c0bb3c",
      "3dba661d68954b97ab103de1c44dc4e4",
      "5bd18ce61e424dfdbadd4c646fe747ac",
      "1598a69c9d4243f6bf8a37c2909de0c1",
      "c0361f10090942e3a682693182e9c9fe",
      "cd4eb6a9790e475e9f27e4168544361b",
      "a58da5a412e14c31afdc56291e00d649",
      "35e9a7ea6003448cbcbffbcfa858f405",
      "a58560ebfe0443ab859b609868385a9f",
      "e0f5a6e38b2747e39ff0262df29faf11",
      "1a19b33063344565b2b6a886a12672b6",
      "c75daebd1b3645029b6501597eefe6f2",
      "3b50069b5c714ecc9bcb9cd3ee5238f2",
      "c4051f378cbe487bb82fad5e0c295250",
      "b945aefb01eb497b80d20ded190e727c",
      "24c8ac6590a646dbb093024040815463",
      "760d6fabeec643b0abb9a42ef431c0b1",
      "15a949d35d504ea6864a0b053620f63e",
      "0d7cb3902835410a8f3dbe5fd22ffca2",
      "10b07153e48d436d91343550f195f2c2",
      "b31b4ca1b911483a954cceb3ff6fa1ce",
      "56d4fa361a2c4327b06559b958ae03cf",
      "84b2ce7926664a26bf240c8134e6b7c3",
      "c7d5dc27e2324c119573fa3ea3c531ad",
      "45cc2e38f052486cacc8d17fa502764a",
      "3fddd71e1b624cdeba94e625b1969f7f",
      "35ee518ca9d94effbb7d069d88f14e23",
      "9c9d44a3f0004282bc14dc33bd24d68d",
      "8232a378d6dd4fb5836f86c6bfc466a1",
      "a4cbc793b91b4c3b9fa657755f99050d",
      "5b71baf9b15d420785e68e5f5466a24e",
      "2a8a8ea4c2ff473ea7efd6bb81ffe83f",
      "37ab95f550a345789c0111ded4d8ca41",
      "11198b92b72a4951894084f2eac8ac94",
      "731b3c54d7224522863b3db95915e48f",
      "3c42f7eb3b99464598735c982ab594bf",
      "a34f7de7be4e4817a1a5516f0ed94175",
      "2900c40a6b264846b90079c0800a30bb",
      "e790716123a84d8cab729413d56a1a9f",
      "f19c07e7f20542c6ae564c9673bedff1",
      "9fef8ac309d149b3915cc4fdba6828d3",
      "f1e85154c08041b68f52b7e59fd2d028",
      "7bfbba60b3a747439381cdcb019d1846",
      "4d255e13daf741e39b1eadc882d98b40",
      "40ba5bbbca9f40fea7da06aad0e18675",
      "fd1f91dfc6e546439aba42a895f55acf",
      "2a3896ec0b2c4b049aa1ffdb6c24f9cb",
      "f56e2fbf22534918872d2cac4fa96133",
      "2cdb80dee85d47a198d8b5c35db1389a",
      "9926d299682e45c2be048c6ef3c18814",
      "225a9dbed6a448569f22623e45c2a13d",
      "55d024d5c64e4fb49e3a866048de03a8",
      "9f6e3bb6f4294717b1149add83f14f00",
      "e34f600d2b9546d0a9ec4537559f0fc1",
      "38f02f3efad3414d9bdffeb9252e26b3",
      "987a6d40d19b4697940c55de295e4a54",
      "2d8fb3ff636845f69078718b96be61bd",
      "0390ad555518491a80dda0ffaec1bfa6",
      "7f530409ff8b4fde847cfe9c9491b65e",
      "ba4623167ab64e60a3b9ce97efee5e06",
      "4ae8da55f8c04b1381391f654e0cd9b7",
      "a18da7dc53f34f7383ba9d94f05fb8e6",
      "de06c2facf2a4985b6efe4bd83ac34db",
      "d7d5a9c7fb8749e9b9ca7a7709a08a3d",
      "d5a81c38e9bd42fea1568eb90924bffd",
      "38e383649e454b5c9c98e499a5f63a8e",
      "4149347a197e418e8fa091ad86b81f71",
      "c195dba0e64646cc923b1d09910e5b66",
      "71fdfbaf230141229ccf14e9e246ae0a",
      "55a5c1531df841918fbf729f4b5d922a",
      "99412cb69a414f9e833a5289a1c09dff",
      "89deb9fba48e4dd5a361c1447351f418",
      "0e936b0ec1234332b591cb3888ca7d57",
      "ed811751cc554f68880f765324a8a0e4",
      "79d470d4d1e144d686d39cafcf768a31",
      "fe8f98598a004463bb09b3d9a1825ecc",
      "d94475477eac4a9584ca50a181a18ade",
      "a064afe4c3ec44d19805218997a10383",
      "b733c114284c4f52b118b1604236b28e",
      "3f15a7cfad114bbf8c5dbd6efd3db67e",
      "3f6dc5cbb1174ee8b66fb5db5c8b9a02",
      "f37072c27cfa4ce9afb78ed48d3060bb",
      "064f783d919047a5b2213eb8bd10cd4c",
      "3cd90a2966a44972856f9bf1f6edda43",
      "a540907bb90e4a8ab45021d56f68af9a",
      "0f8c06bffd4341c3928e2917cc46aefa",
      "9f06358a4c3c4e8b82dd1ef6e5d584d7",
      "7e136f81a6014547b74e474ea8ba38f5",
      "ebde1637480c4bf49420fa083e4bf726",
      "9f9bb3b6a7e147a89a31ee1df9ea590b",
      "862fb52bc8da4bdfbe777c790855df07",
      "f31dbbdbae3645cdb8395eae20a05ee8",
      "b761d08c3636461bb423074bbd8eb191",
      "d96b00323a604489aa540258a29c2e94",
      "9c01fe8c44aa40d8891b1c1290223c1b",
      "7571b95b0d9d40c6bd79068ed9ef568f",
      "367cd36545ab4d7382fb282151204fdc",
      "61cc74c54b6b453d8cc76bce67a5d6c2",
      "5a2106b1d7254fdd8fc7013d8d3c4c61",
      "f33325ed5fbc47f0ba4293267531d71a",
      "61eec9b14e9e4a2fb8e4bf1a5c817e32",
      "d3bb51ef756b44059bb69da6f3b901a1",
      "0ff5f685ab404d6f8549706f7484da5e",
      "36ed7cf6d6e74301be8b613f13e2271f",
      "dc675b17782c48d880bb4a367fb09db4",
      "df7b2eef3ce140258566eda7e15bd92a",
      "5d8e0c744cf14ee2be2fdded40a7cdb1",
      "11f48d70d9714992a8907527eb9cd59d",
      "80d6698e07344ee6828105f2dcde5cdf",
      "2f8c003ebff143ef9ebd01a8b35a3bbe",
      "aa794cfd82684af6bd7888ba90372b1f",
      "254223785cc5449ab9484a790160c4d5",
      "e054becd5222453b842382244345eaa4",
      "df37edf4bc2b44409bea4d53abcb9d15",
      "7734606b35474ca6aa4fd5db169c9608",
      "56c60e1eee8e46aa932c5813c96029a8",
      "28b0eea0f8714c0fb3159d8f4a94421a",
      "cb449e211f5a4d28a0a574eb8e9c4c82",
      "3c67270afa6f4d879c933243e1399730",
      "930739acb4a84e4f9ca20442bd5747fc",
      "bf9bda3c1ec64a82a52c35e0853f9dbc",
      "c979a587f5bf4a42baa9b2546b321c1b",
      "c645a1e3188a4533b8d13381f7dd8938",
      "7f75a5ff38f746d0abe206fca4467d76",
      "23e37b8978d94034a8edb824ee84e839",
      "ddc7b90da1b84659a627d907ccb5389c",
      "9eb4e12ff0604bb485841a147997827b",
      "24cb9d355c364a05ab9becf01fa54a8d",
      "d50f3d104dc74eaaaf50170bdb4ba380",
      "2d1389ed50734afcb42ffa45d9385673",
      "f0482b2f464844d3be92d6ef2907ecd2",
      "c31b1f55188448c6ab0e2db146d506c2",
      "011bc0d0a6ec42dca8dcdf10923d3f6c",
      "0279090be16049b28bb462e91150612c",
      "ea618d4677f24f0e9cffbdd33c582eeb",
      "6ba502a103f4417f879009ec20f99c8c",
      "bec8ab85329c4334a64cba9f299b6d26",
      "b7cf27051dfa42119bda04b54662d88d",
      "ed901822e2d7413990f177aba8ab5568",
      "bd8cb50ce90842ad9977650bedc5991d",
      "ced5f662bc984c49b3f09ed59185a57b",
      "e76c3fe3f423456db23b3047a6ea85df",
      "85684e81b2ee45f6a60244e2ab17624b",
      "951efe7d0355490ab8e4f2077f2f8b0e",
      "8ff27a039ed2444d967d500925900aa2",
      "682dbeeab2b047c6aeae370608d58452",
      "cd939c43af4f49189efb76db53dec1b7",
      "41aaf0af875743c69d42a6546025dbad",
      "a221e885ca664c2986b9e1ff682218d8",
      "50931f7cc9ba4b63b1a9511428a7c12e",
      "c4721c5727fd41ae84e8ede67adebe76",
      "6341056a01b742eaa533562371da1927",
      "a0bfc1f21efa4b8a8aac8dcd862d63e3",
      "97657a8018574ae498ecde26811d1730",
      "6d171a4550ca4361b04652732ddbfb0e",
      "689ddec275ca49349b15f06886e503e9",
      "b69001cd02a04a02a5d41a71233f0645",
      "9f039bd187bb454eb5a0f77851dfae77",
      "b80077cd5f1849c1886f68b3b500a19e",
      "3aa4a109dc62498b8714fdce3fb9f98b",
      "1086488a692749f89f8d72e3890d0836",
      "4ab273fbf2474ccda1cf873561ee9aea",
      "323dce5111764a04b5e42ab6298a0653",
      "8eeb4dd7f80b430cbda26cde4da81806",
      "2efa346328a84067bfcb2779ff20806f",
      "b521f4b7b51c4b8faaa7ce13c69dc8ac",
      "379dec3138334429a70cd8a4b4aae46c",
      "9c9302d4882b441d94b3f1ac75e455e7",
      "b148e34abfe64e8383b6392f05f84f60",
      "2df3551603e24f73a067399a7cd8fbe1",
      "31881548797d40d0948f28b042f7f466",
      "de1da72154f941d1b7828887dfc26093",
      "dd9ad7eb2a5b40bf85514bf1e1a73776",
      "7025878a4989409cb1c0c53726bad855",
      "6b652e7be2a441289505c24de14eec55",
      "f0c5849446fb4e42a703f40bf751433b",
      "3453e2efa62949b1856d52b6df033c77",
      "215ddc4bd05d4ed4adac294672bdd157",
      "347873f1ac924585a7ceafda7f2b1bff",
      "2a992ba9cdd14f5391808879d144f7f5",
      "c84b6327514e486191854e3733f067d9",
      "f0e3f1f9ba2f4d23a0975fd072ebc14e",
      "4bbb9922073f40d385f84c7658f267a3",
      "61679fdcda404b14a2638bc25104fdf9",
      "51eec3f4f41f40e8abf50a592903150a",
      "4e737a3781cf46af94cb50d94bfd4a66",
      "cb07e6606325493d9656a81e23db0967",
      "cfd95ee761af4f2e90a2ce5653d8a693",
      "d3974315244c40779835a8880be95877",
      "c3127941cafc4afc95fe54593936f0ff",
      "0fcbd10c4f5e43cda3458f3e6924f797",
      "296475bbd0a7492992b8a56577adc14e",
      "a1d52ce68f67475fb295dddf83699a60",
      "9d2909949ecd4e89bdae0f3e2b2571ab",
      "b4e099b86e7d4b278e66d476faff8027",
      "d123c34b1e2a426b98b1181b4f7233b9",
      "c0d945b1c33941fca3c742b0b1417719",
      "0d0bd01f24fc49dcab90576d9a41e61d",
      "8282a9a691304270beb0460b209ceba7",
      "c6e9836375644e5685960ae81afdf337",
      "fcff4372790740319a1c983eea7e625f",
      "2c7258b6420545a4ad746c02aca31b2d",
      "e8bd67a8a8dc41aca72af68f39fbcb32",
      "a4cc43475137477787adea896b4f9190",
      "7fd431e535634c16b054f4b9fc8d3fdf",
      "e9f1e2b07bb547c6bd85d540aa3a06ec",
      "5e68533d96ba4e07a2f252205e879ba0",
      "5ad675d8926749ae83e260bda5a54b49",
      "b00ea3179e4b41ecb25510838090f7c9",
      "6cc1ebf1f7774552bb7ad9e2f2dc3d42",
      "be36d14b62c24b15a91c64f65d7c091d",
      "6a97962826f4492e909baa91f022ea41",
      "c44adead2d55451db86a0571dfb2c23d",
      "add2a7a0030f4afa9fc93616696e4444",
      "48f4ed05556849268db2824b53db8da5",
      "e1a7d824a4e44ed29a907b1b0c0690ed",
      "9a1803b9528b40d7af59ccd9ca9c0526",
      "31001b4c2faf4d4facb24f8222442105",
      "54e7bdc6b24b4dc2a83a1ea247db1ed7",
      "37215d465a8040309d09459b0098dd90",
      "c840c00111cd43748cebefbca08dc065",
      "54407f771fc04d29863645a8c544477b",
      "740fe9b2398641b1bd0a6a9376e79c1e",
      "83c3ac6e776e4bad87d669a9e8d9e711",
      "bd8f7f532bb14704b78f54bd4a16821b",
      "6981eda681d54ee5bb675ac063a8989f",
      "cb937413110646e8a2662330c524e47c",
      "232023e7fcd34aa18cd21285959e54cf",
      "0280385a1d294870bc99264b44bd1302",
      "d7a17c48f68745649a141cfe770b9553",
      "eadfa3ed4e7c4822929caa15230f6a08",
      "5a768b8543d44b3bb6eaf396e8a324a5",
      "4fb64f46a8854350b8184078475bf448",
      "63ebf8e1663743bc9f98c8f582895546",
      "a7133ba9b98f4ced8c5f0a7bd05829e5",
      "c8d1ec2ab3aa4d93a06188e0d3daeb23",
      "cfe37156359e4cae9b02f86d231b5239",
      "12248a4ba4694ef68c14985cbbe24928",
      "a631f30f05dd4518a1b37bd5f038c6da",
      "ab521023aaf04f8d9a3849a853003111",
      "45b016882a4e4e00897e74ad72467dd9",
      "7c964fa678d34c8f8a5bf7a9c4f97076",
      "39f3fb510cd645c3afe7d12714574cab",
      "363744b9f36a42309a5c500d37bc01f7",
      "63603bf937264dd79b0498b4c39ad812",
      "c66e8cbee9a8461a9527fe97d572a03e",
      "80929a25ee094f189ce1d1ece5ae27c4",
      "e2273925b9254ec785a14ac6e8513493",
      "19c2d2b775084187ab4bed63ee8a5351",
      "7d3f67e49a5540efb5a3fe3eff0dd2cf",
      "604741b3442d43feb4016f0746afbd72",
      "131e91bb10354b8ebac714b5c0130238",
      "608f14bf43154a6995b8b4057fc5ce95",
      "9e4381638999470fbe1d8fe2702b4cf2",
      "1e010b2607f94f1f9063a769a6659ea3",
      "6ed43e5374ec4f6badb1e94c45c522bf",
      "e2f5904b01a941788dcf8267139bb8e2",
      "c69834b896054fef945644308ae149ad",
      "4cb7a4939036455d8effd68c7a3f9dfd",
      "07af173c718644d1bddad5e0ba24d957",
      "5a5a249b70c34b74b27a29d0bbbab13d",
      "30c5044dcb6146ac92daf8d4d11aa70c",
      "2abcad88472a48e0a8e3121570d16e50",
      "b42432cb720b4e5cbfb977b8463165ff",
      "4c1f0faa1860445aa94a6a1f5d4103aa",
      "01f488a7dc4e4d0896b362dbd64cf812",
      "b76531405082437b8c2d13ae7800f2be",
      "7a8dac64dabe4752b7b58ec6e4dec132",
      "72715463a159403c9237bcf10332ed6d",
      "15c8cafc4ba646638d593a8c50f5c2fa",
      "d55994c39c544e01b474cb477ed66ad7",
      "49f7934a10214518a1fd658da61b64ac",
      "c2a5943ed6ea4d40a2acfd792d9c8406",
      "7353782dcb1d4f33849b35fabc9565ed",
      "473a401cff5b447f88beca450ec9a04c",
      "eecbc1af9d294da297254a913134e6b2",
      "f6df996af01246ad9ba93f3113cb371f",
      "f65082e9d88e42a6996984fc68ebc0c7",
      "7f2719269df043be91d0ad868e13a23a",
      "53594f27254143cdb5365806e99a8425",
      "eef9383ddc2f4b35b95df986ae25dae0",
      "d3082fe76df042b2bdf252d58be7f16b",
      "880bdc4670634d34913ee966f5f1d569",
      "6f3a627fe8224ba49f18366dbbc1f7d8",
      "8387f1d303c3498eb7a9e6a2c57d3708",
      "82dfe8bed3054e789f7fe70a4cc0c7d7",
      "44979e94fb814fa4af9e6f79bf819ddc",
      "af9fa94687324186add630c33c2640cb",
      "55b163f9a5864d71b2034c31f74c7538",
      "9d08811ced624f2fb9636703dba7d588",
      "192da98d46d64816838c2679628da8df",
      "1da02994d86e41cdb9f0228791afcc4d",
      "00d39b4efd1a435eb937a6674e183312",
      "6c54e92ed4354e0ea4f32e1b802c4618",
      "7f02d3389df74c3187994f5074708d69",
      "9ea202c6f7344322872de34dd6d6c514",
      "05a45fe75f0e456fb7ebdf001148e413",
      "465b9d3754eb4b08baf2cb6da52f67ba",
      "e8d62178cebf4236ba6da2f59713f8d9",
      "1169058c7f9747f2855915ace7bb2386",
      "1351e33133f1483bb8d7a21201e923f6",
      "10873dbfdfb249d782c48baae385339e",
      "ec6a8224c4ed4e87928cec28fcabc7ec",
      "f7c02e6222bd4f8580ccd0a3482f6dba",
      "3de3030856844bd7bf4133c44fc74d48",
      "51aa33ad4fce420fab05478c383336fe",
      "0334983b238f48739c061cf37648dda0",
      "b784e894e7054d4da8f229af9fe3a639",
      "e3c8bf603ad943cb9abaa854f5dcfc2b",
      "12bc53e627014ef2ad1a25c851d15f77",
      "5a48339b2b62469495a46060560102d1",
      "0a7f0512cef64090bc5aae9c1a42f9ee",
      "bb897e98c2704042b1c7ba454fb668ea",
      "a6fceaa3190846f2bbb07f968b9509d0",
      "a5d9c94a98a54a87b264527c6d595ccc",
      "4902f6978b1041fdbee3be6c80d52251",
      "678f79fb7c1645e79e1eb1c2a7c03ebf",
      "5917ac4e14f14517a57274957ff39436",
      "2dba3a4e98584f1d8751dc40ffc42309",
      "57fe6ce356554813b5315c7463f28eb4",
      "2f12e2bde34f447dbe772c988bcb360c",
      "d2342dbee1a04b2f81ffc9ddb5f4a541",
      "95f8c35c823042b6a48fe56ffabab25c",
      "7f68b0d0895c4421b34282d9e4e19b98",
      "06f2df816a9f440bb7b24e6714484fdb",
      "f3a1fb99b29e4a809dcbe8240dd5b04e",
      "3ff1416ae59d44de8991230e82aa480e",
      "76743ed5c1eb4320a03ae18ef964357d",
      "d1a0cd4b4efe46a69b029a1fb56ddd2a",
      "e7567933b184460fa697cf56d7da9752",
      "791c7dcc6193407a80d898d6f5d7cb54",
      "3e4e5af6c83d412da2fcae5fa6679e4d",
      "bed9746bfa6744f1b74840f853dcc44c",
      "d81a84c0945248d4857f063a1d8aa3b3",
      "052f846172114886bd2483c958d77d6c",
      "139e6ff0f79f4f2dae44fd75dc79e908",
      "d8943671239843cbbaf728b190dc2279",
      "8cd15f0b837f42a5b0bf79aac3fced31",
      "7ebd0ce504994e08b2fcd098ac1c582c",
      "94a59e804c44497a8aaefaa0f2631bc6",
      "d1a2fd76361d4d83a412632f0aa893c2",
      "f5b02ee2b12e423d8c5fe35880de73b9",
      "8adf39e5e5354984babf18905212f0c7",
      "1b3e7637d0c04778a06cc51e464b446a",
      "ad2e558ad1584157ace726433fe5baf2",
      "673e2b93267544df946821649f6b0ece",
      "918771f05440452380fd7e32700a1ce1",
      "c1a3d69bddf7461287e9b6de4475bc10",
      "fe995b2fa91f4c9bb3ce6498a1cd178d",
      "1d265036d9094359b9ab62b4491a4366",
      "bec3820352df486e93fc123dad3df80e",
      "daf17e16b8c04f89a07f10cd5142dda3",
      "21718e5bf03f46ad9f89f51f4b989376",
      "3e6f5cf5eb1c424ba24ed5722876baf6",
      "c4ae4e2984664419940926c45559c06a",
      "81f6129c91de46669220108115d57c99",
      "721f67f579ac4659a47905e9d78eba61",
      "1abd3a2b9a354c699133a431bc0cb92c",
      "898618e06f40409ab51dc95c8309a2c0",
      "a83cf76750e74fafa5bf46edb8626270",
      "97bb3767fdc742178fc8bd58d49a47ad",
      "12c66e711b6449068146364b9593b1dd",
      "74ca8905a0694db1acd3b115d6c853a3",
      "802c432a3e9543b6997455bcf95a6f95",
      "7dc10cea9769491fbe07a8d633324bbd",
      "a78616c0de1e46058ce0dc23cde81e81",
      "baeabb1b428f4498812616988e0844c0",
      "0c5b63105ef84d80aade46f93077a4c8",
      "eae54bad8d94421faa6af242af16f903",
      "62924312338f45399cb58c1d39798647",
      "81fb3b1148554e47b21f20b3096e88cf",
      "59d9267409c64457a9a0281651175728",
      "e3eaaf42a23d46ae9b53a6b50bfacd28",
      "6382fa57f0e64e919b80c78ee47bb918",
      "faf089824d5f4bf395f31c18aac3be20",
      "74a192ecf83b49de9eea4340c86dc87a",
      "48c763355dd14f7e9c9ec0a8cc5160d6",
      "b7731c9fdf404c0181b4401942de4890",
      "f3907f60b10f4fdcad32ad7b034ff52f",
      "6f0209f79fb847efadf9a47d1196dd4b",
      "a6440b0b424f4b68b85a8b74aff0b523",
      "a3285abc37c14c8a905d370819adef48",
      "3cf1943a4c544b258d1f71ad816f5f68",
      "35ee7b3d8bd34235bbb2542d7d196b46",
      "3de7a0a3ce4b459389032cd6c19dd584",
      "81b57da2919e42478a7ac881a45902d5",
      "84a9a1a99c3e4c6aa7bd58c7802b982e",
      "755cf407afac4479831afbf638e836c7",
      "c01034f9d4664beabf9dd9b6d2c66ee2",
      "5c155947587a426fafae264756488b02",
      "5746b9b62bca43278349b359d54e6130"
     ]
    },
    "id": "Ttjm5SsWe01P",
    "outputId": "196a8e3c-bf87-463e-ccc9-2285b811ec47"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Evaluation Loop\n",
    "# =============================================================================\n",
    "# This cell runs the complete LACE evaluation pipeline:\n",
    "#   1. Iterates through each model in the configuration\n",
    "#   2. Loads the model (with quantization if needed)\n",
    "#   3. Evaluates on all three datasets\n",
    "#   4. Computes all six metrics plus the final LCS score\n",
    "#   5. Cleans up GPU memory between models\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def reset_cuda() -> None:\n",
    "    \"\"\"\n",
    "    Reset CUDA state and free GPU memory.\n",
    "\n",
    "    Clears the CUDA cache, synchronizes all streams, and runs garbage\n",
    "    collection. Should be called between model evaluations to prevent\n",
    "    out-of-memory errors.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Begin Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING LACE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name in CONFIG.models_to_test:\n",
    "    print(f\"\\nEvaluating: {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Reset CUDA state before loading each model\n",
    "    reset_cuda()\n",
    "\n",
    "    try:\n",
    "        # Check if this model requires 4-bit quantization\n",
    "        quantize = model_name in CONFIG.quantized_models\n",
    "\n",
    "        # Load the candidate model\n",
    "        llm = CandidateLLM(model_name, quantize=quantize)\n",
    "\n",
    "        # Initialize metric scores to zero\n",
    "        ec, cr, rs_f, ac, rs_kg, kas = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Evaluate on EntailmentBank (EC and CR metrics)\n",
    "        # ---------------------------------------------------------------------\n",
    "        try:\n",
    "            ec, cr = score_entailmentbank(llm, subset_eb, nli_judge, CONFIG)\n",
    "        except Exception as e:\n",
    "            print(f\"   EntailmentBank error: {str(e)[:50]}\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Evaluate on RuleTaker (RS_F, AC, and KAS metrics)\n",
    "        # ---------------------------------------------------------------------\n",
    "        try:\n",
    "            rs_f, ac, kas = score_ruletaker(llm, subset_rt, CONFIG)\n",
    "        except Exception as e:\n",
    "            print(f\"   RuleTaker error: {str(e)[:50]}\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Evaluate on WebNLG (RS_KG metric)\n",
    "        # ---------------------------------------------------------------------\n",
    "        try:\n",
    "            rs_kg = score_webnlg(llm, subset_wn, nli_judge)\n",
    "        except Exception as e:\n",
    "            print(f\"   WebNLG error: {str(e)[:50]}\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Compute final LCS score\n",
    "        # ---------------------------------------------------------------------\n",
    "        lcs = compute_lcs(\n",
    "            ec, cr, rs_f, rs_kg, kas, ac,\n",
    "            weights=CONFIG.lcs_weights\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"EC\": ec,\n",
    "            \"CR\": cr,\n",
    "            \"AC\": ac,\n",
    "            \"RS_F\": rs_f,\n",
    "            \"RS_KG\": rs_kg,\n",
    "            \"KAS\": kas,\n",
    "            \"LCS\": lcs\n",
    "        })\n",
    "\n",
    "        # Print summary for this model\n",
    "        print(f\"   EC={ec:.3f}  CR={cr:.3f}  AC={ac:.3f}\")\n",
    "        print(f\"   RS_F={rs_f:.3f}  RS_KG={rs_kg:.3f}  KAS={kas:.3f}\")\n",
    "        print(f\"   LCS={lcs:.3f}\")\n",
    "\n",
    "        # Clean up model to free GPU memory\n",
    "        llm.cleanup()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to evaluate model: {e}\")\n",
    "        # Record zero scores for failed models\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"EC\": 0.0,\n",
    "            \"CR\": 0.0,\n",
    "            \"AC\": 0.0,\n",
    "            \"RS_F\": 0.0,\n",
    "            \"RS_KG\": 0.0,\n",
    "            \"KAS\": 0.0,\n",
    "            \"LCS\": 0.0,\n",
    "        })\n",
    "\n",
    "    # Reset CUDA after each model\n",
    "    reset_cuda()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluation Complete\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELzso9l8e01P"
   },
   "source": [
    "## 9. Results Table\n",
    "\n",
    "Display evaluation results in a formatted table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "PT9saSzGe01Q",
    "outputId": "1fb433fa-8993-4090-ad40-29688f93207c"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Results Summary Table\n",
    "# =============================================================================\n",
    "# This cell creates a formatted DataFrame of results and saves to CSV.\n",
    "# The table shows all metrics for each model, with LCS highlighted.\n",
    "# =============================================================================\n",
    "\n",
    "# Create pandas DataFrame from results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns for consistent display\n",
    "column_order = [\"model\", \"EC\", \"CR\", \"AC\", \"RS_F\", \"RS_KG\", \"KAS\", \"LCS\"]\n",
    "df = df[[col for col in column_order if col in df.columns]]\n",
    "\n",
    "# Create styled DataFrame for display\n",
    "# Format numbers to 3 decimal places and highlight LCS column\n",
    "styled_df = df.style.format({\n",
    "    \"EC\": \"{:.3f}\",\n",
    "    \"CR\": \"{:.3f}\",\n",
    "    \"AC\": \"{:.3f}\",\n",
    "    \"RS_F\": \"{:.3f}\",\n",
    "    \"RS_KG\": \"{:.3f}\",\n",
    "    \"KAS\": \"{:.3f}\",\n",
    "    \"LCS\": \"{:.3f}\"\n",
    "}).background_gradient(subset=[\"LCS\"], cmap=\"RdYlGn\")\n",
    "\n",
    "# Display the styled table\n",
    "display(styled_df)\n",
    "\n",
    "# Save results to CSV file\n",
    "output_filename = \"lace_results.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nResults saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPO9HNeTe01Q"
   },
   "source": [
    "## 10. Visualization\n",
    "\n",
    "Generate comparison charts for metrics and LCS scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "KQoVQjVge01Q",
    "outputId": "35cc9815-c4ef-46cf-f4a5-0baf79569f46"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Results Visualization\n",
    "# =============================================================================\n",
    "# This cell generates five publication-ready plots:\n",
    "#   1. Grouped bar chart comparing all metrics across models\n",
    "#   2. LCS score comparison bar chart\n",
    "#   3. Heatmap of all metrics including LCS\n",
    "#   4. Stacked bar chart showing weighted LCS contributions\n",
    "#   5. Stacked bar chart showing unweighted LCS contributions\n",
    "#\n",
    "# All plots are saved as separate PNG files for use in reports and papers.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def generate_visualizations(results_df, lcs_weights):\n",
    "    \"\"\"\n",
    "    Generate all visualization plots for LACE evaluation results.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame containing evaluation results with columns\n",
    "                   for model names and all metric scores.\n",
    "        lcs_weights: Dictionary mapping metric names to their weights\n",
    "                    in the LCS formula.\n",
    "\n",
    "    Returns:\n",
    "        None. Saves five PNG files to the current directory.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Setup common variables\n",
    "    # -------------------------------------------------------------------------\n",
    "    metrics = [\"EC\", \"CR\", \"AC\", \"RS_F\", \"RS_KG\", \"KAS\"]\n",
    "    x_positions = np.arange(len(results_df))\n",
    "    model_labels = [m.split(\"/\")[-1][:15] for m in results_df[\"model\"]]\n",
    "\n",
    "    # =========================================================================\n",
    "    # Plot 1: All Metrics Comparison (Grouped Bar Chart)\n",
    "    # =========================================================================\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bar_width = 0.12\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        offset = (i - len(metrics) / 2 + 0.5) * bar_width\n",
    "        plt.bar(x_positions + offset, results_df[metric], bar_width, label=metric)\n",
    "\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"LACE Metrics by Model\")\n",
    "    plt.xticks(x_positions, model_labels, rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lace_metrics_grouped.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved: lace_metrics_grouped.png\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Plot 2: LCS Score Comparison\n",
    "    # =========================================================================\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.8, len(results_df)))\n",
    "    bars = plt.bar(x_positions, results_df[\"LCS\"], color=colors, edgecolor=\"black\")\n",
    "\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"LCS Score\")\n",
    "    plt.title(\"Logic Consistency Score (LCS)\")\n",
    "    plt.xticks(x_positions, model_labels, rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, results_df[\"LCS\"]):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.02,\n",
    "            f\"{value:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lace_lcs.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved: lace_lcs.png\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Plot 3: Heatmap of All Metrics\n",
    "    # =========================================================================\n",
    "    heat_cols = metrics + [\"LCS\"]\n",
    "    heat_data = results_df[heat_cols].to_numpy(dtype=float)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(heat_data, aspect=\"auto\", cmap=\"RdYlGn\")\n",
    "    plt.colorbar(label=\"Score\")\n",
    "\n",
    "    plt.xticks(np.arange(len(heat_cols)), heat_cols, rotation=45, ha=\"right\")\n",
    "    plt.yticks(np.arange(len(model_labels)), model_labels)\n",
    "    plt.title(\"LACE Metrics Heatmap\")\n",
    "\n",
    "    # Add value annotations\n",
    "    for i in range(heat_data.shape[0]):\n",
    "        for j in range(heat_data.shape[1]):\n",
    "            plt.text(j, i, f\"{heat_data[i, j]:.3f}\",\n",
    "                    ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lace_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved: lace_heatmap.png\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Plot 4: Weighted LCS Contribution Breakdown (Stacked Bar)\n",
    "    # =========================================================================\n",
    "    total_weight = sum(lcs_weights.values())\n",
    "\n",
    "    # Calculate weighted contribution of each metric\n",
    "    contributions = {\n",
    "        \"EC\": (lcs_weights[\"ec\"] * results_df[\"EC\"].to_numpy()) / total_weight,\n",
    "        \"1-CR\": (lcs_weights[\"cr\"] * (1.0 - results_df[\"CR\"].to_numpy())) / total_weight,\n",
    "        \"AC\": (lcs_weights[\"ac\"] * results_df[\"AC\"].to_numpy()) / total_weight,\n",
    "        \"RS_F\": (lcs_weights[\"rs_f\"] * results_df[\"RS_F\"].to_numpy()) / total_weight,\n",
    "        \"RS_KG\": (lcs_weights[\"rs_kg\"] * results_df[\"RS_KG\"].to_numpy()) / total_weight,\n",
    "        \"KAS\": (lcs_weights[\"kas\"] * results_df[\"KAS\"].to_numpy()) / total_weight,\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bottom = np.zeros(len(results_df))\n",
    "\n",
    "    for label, vals in contributions.items():\n",
    "        plt.bar(x_positions, vals, bottom=bottom, label=label, edgecolor=\"black\")\n",
    "        bottom += vals\n",
    "\n",
    "    # Add total labels\n",
    "    for i, total in enumerate(bottom):\n",
    "        plt.text(i, total + 0.01, f\"{total:.3f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "    plt.xticks(x_positions, model_labels, rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, max(bottom) * 1.15)\n",
    "    plt.ylabel(\"Weighted Contribution to LCS\")\n",
    "    plt.title(\"LCS Component Breakdown (Weighted)\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.legend(fontsize=8, loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lace_lcs_contributions_weighted.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved: lace_lcs_contributions_weighted.png\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Plot 5: Unweighted LCS Contribution Breakdown (Stacked Bar)\n",
    "    # =========================================================================\n",
    "    equal_weight = 1.0 / 6.0\n",
    "\n",
    "    contributions_unweighted = {\n",
    "        \"EC\": equal_weight * results_df[\"EC\"].to_numpy(),\n",
    "        \"1-CR\": equal_weight * (1.0 - results_df[\"CR\"].to_numpy()),\n",
    "        \"AC\": equal_weight * results_df[\"AC\"].to_numpy(),\n",
    "        \"RS_F\": equal_weight * results_df[\"RS_F\"].to_numpy(),\n",
    "        \"RS_KG\": equal_weight * results_df[\"RS_KG\"].to_numpy(),\n",
    "        \"KAS\": equal_weight * results_df[\"KAS\"].to_numpy(),\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bottom_u = np.zeros(len(results_df))\n",
    "\n",
    "    for label, vals in contributions_unweighted.items():\n",
    "        plt.bar(x_positions, vals, bottom=bottom_u, label=label, edgecolor=\"black\")\n",
    "        bottom_u += vals\n",
    "\n",
    "    # Add total labels\n",
    "    for i, total in enumerate(bottom_u):\n",
    "        plt.text(i, total + 0.01, f\"{total:.3f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "    plt.xticks(x_positions, model_labels, rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, max(bottom_u) * 1.15)\n",
    "    plt.ylabel(\"Contribution to LCS (Equal Weights)\")\n",
    "    plt.title(\"LCS Component Breakdown (Unweighted)\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.legend(fontsize=8, loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"lace_lcs_contributions_unweighted.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved: lace_lcs_contributions_unweighted.png\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate all visualizations\n",
    "# -----------------------------------------------------------------------------\n",
    "generate_visualizations(df, CONFIG.lcs_weights)\n",
    "\n",
    "print(\"\\nAll visualizations saved successfully\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bv30XhBe01Q"
   },
   "source": [
    "## 11. Metric Interpretation\n",
    "\n",
    "| Metric | Range | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| EC | [0, 1] | Higher is better - model outputs entail gold answers |\n",
    "| CR | [0, 1] | Lower is better - model outputs do not contradict context |\n",
    "| AC | [0, 1] | Higher is better - model gives consistent answers |\n",
    "| RS_F | [0, 1] | Higher is better - correct logical reasoning |\n",
    "| RS_KG | [0, 1] | Higher is better - accurate knowledge graph verbalization |\n",
    "| KAS | [0, 1] | Higher is better - sensitive to counterfactual changes |\n",
    "| LCS | [0, 1] | Higher is better - overall logic consistency |\n",
    "\n",
    "### LCS Weighting Rationale\n",
    "\n",
    "The weighted LCS formula emphasizes discriminative metrics:\n",
    "- **EC, RS_F, KAS** (weight=2.0): These metrics show the most variation across model sizes\n",
    "- **RS_KG** (weight=1.0): Moderate discrimination\n",
    "- **CR, AC** (weight=0.5): Near-floor and near-ceiling respectively, less informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bQtMhjfe01Q",
    "outputId": "b3cde5ca-7864-4484-f5cc-3b0f5084e784"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cleanup\n",
    "# =============================================================================\n",
    "# Release GPU memory used by the NLI judge model.\n",
    "# This should be run after evaluation is complete.\n",
    "# =============================================================================\n",
    "\n",
    "nli_judge.cleanup()\n",
    "print(\"\\nCleanup complete - GPU memory released\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
